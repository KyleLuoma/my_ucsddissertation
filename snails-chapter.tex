\section{Introduction}


\begin{figure*}
  \centering
  \includegraphics[width=\linewidth]{figures/Naturalness Pipeline - modification_workflow.pdf}
  \caption{Databases with poorly named, or less natural, schema identifiers perform poorly in LLM-based NL-to-SQL interfaces, and this project exposes the need for more natural schemas. We offer approaches and artifacts, including a naturalness classification and modification workflow, that can aid in the naturalness assessment and modification processes required to create a performance-enhancing natural view. In this way, the native schema remains as-is so that existing tools can continue talking to it without modification, while an LLM-based NLI can be integrated into the existing stack via a natural view.}
  \label{fig:artifact-map}
\end{figure*}

Natural language-to-SQL (NL-to-SQL) query generation capability has been revolutionized by foundational large language models (LLMs)~\cite{openai-chatgpt-blog-post, roziere2023code, anil2023palm}.
This has made the integration of LLM-based query tools into relational database workflows more viable, with both established DBMS vendors and startups beginning to offer commercial NL-to-SQL interfaces.
However, challenges in the NL-to-SQL space remain that can degrade the effectiveness of an LLM-enabled data retrieval workflow in real-world databases~\cite{floratou2024nl2sql}.
Principal among such challenges is \emph{schema linking}, which is the association of entities in NL utterances to elements in the database schema.

While much work has studied making LLMs larger or more sophisticated, a more basic issue often underlies this challenge: lexical mismatches between natural language and poorly-named tables and columns in a schema.
Intuitively, schema elements that are ``better named'' could raise the accuracy of schema linking within the NL-to-SQL setup.
In this paper, we unpack and dive deeper into this intuition to study how exactly the ``naturalness'' of schema elements matters for NL-to-SQL by instituting a new benchmark and performing extensive empirical analysis using that.
One might ask: \emph{Why bother formalizing a concept that seems obvious and intuitive?} 
We believe this is important for 2 reasons.
First, without a more formalized--or at least automated way--to define, verify, and compare ``naturalness'' researchers and practitioners alike will be forced to grapple with ad hoc and inconsistent approaches. 
In turn, this can lead to confounded conclusions by researchers on how different LLMs behave on different schemas and mislead practitioners comparing different NLIs. 
This points to the need for a new benchmark labeled dataset for this problem.

Second, practitioners need a way to efficiently and accurately operationalize any insights about the impact of naturalness on their schema elements for LLM-based NLIs.
This points to the need for a systematic evaluation of how naturalness affects different databases, queries, and LLMs used for NL-to-SQL.

\paragraph{\textbf{Our Focus}}

In this paper, we take the first steps toward deeper understanding on this seemingly obvious, but hitherto underexplored and important, relationship between schema identifier naturalness and LLM-based NL-to-SQL.
Specifically, we ask the following three interconnected questions.
(1) How do we quantify ``naturalness'' of schema identifiers? (2) Does it really impact schema linking accuracy in LLM-based NL-to-SQL and if so, by how much? (3) How does that impact vary by complexity of the database and query, as well as across different popular LLMs?

To answer the above questions, we create a novel integrated benchmark suite we call SNAILS with new collections of real-world databases and query pairs, a new labeled dataset of schema identifiers, a set of evaluation metrics, and LLM prompting and other AI artifacts.

\subsection{Preliminaries and Setup}

\paragraph{\textbf{LLM-based NL-to-SQL}}

The most obvious way to seek LLM performance improvements would be by increasing the power of the language models themselves. 
But the cost of training and deploying LLMs continues to increase in concert with their complexities. 
Additionally, many practitioners seek ``plug and play'' solutions by employing already-available LLMs. 
Model training and finetuning impose access barriers that may render such a pursuit untenable for organizations that use databases but lack the requisite talent such as data science and machine learning expertise.

The practice of prompt engineering can also help improve NL-to-SQL performance, 
though dealing with schema complexity and schema representations in LLM prompting is an ongoing challenge in enterprise-level NL-to-SQL applications~\cite{floratou2024nl2sql}.
The majority of leading submissions on the popular Spider NL-to-SQL benchmark leaderboard are LLM-based solutions~\cite{gao2023texttosql,pourreza2023dinsql,dong2023c3} that employ a variety of prompting strategies, some of which require multiple successive API requests containing schema context and instructions.
These approaches can be costly and unintuitive for NLIDB end users, and can incur excessive costs and overhead when deployed at scale. 

A complementary line of work on realistic NL-to-SQL benchmarking uses structural schema modification such as normalization, flattening, and replacement to evaluate effects on LLM performance.
Making such structural changes to target schemas challenges model robustness and increases error rates in NL-to-SQL performance~\cite{10.14778/3494124.3494139}, and this recent work indicates that schema design is a viable target for LLM-based NL-to-SQL accuracy improvements.

\paragraph{\textbf{Schema Linking}}

Schema linking remains as a persistent challenge for LLMs.
With the availability of capable LLMs that consistently generate valid SQL statements, a larger proportion of NL-to-SQL generation errors are now associated with incorrect or ambiguous database identifier selection as opposed to incorrect syntax~\cite{49288}.
Schema linking performance has been improved using lexical matching heuristics~\cite{10023434, 10096170}, joint relationally aware embeddings with attention~\cite{wang2020rat-sql, cao-etal-2021-lgesql}, the use of pre-trained language models to perform schema probing~\cite{10.1145/3534678.3539305}, and multimodel pipelines with ML models for pruning schema knowledge~\cite{li2023resdsql}.
Some NL-to-SQL methods address schema linking challenges by adding additional context such as sample values or metadata~\cite{pourreza2023dinsql} to schema knowledge representations.
These methods can improve performance in some cases~\cite{nan2023enhancingfewshottexttosqlcapabilities}, and can be useful for schemas with obscurely-named tables and columns, though they do so at the cost of much larger schema knowledge representations.


Schema linking still often fails, even with the most capable LLMs due to poorly-aligned schema identifier names with natural language question contents, that could be due to the use of synonyms or the obscurity of a database identifier.
In the latter case, it can be challenging for even a sophisticated linking solution to match natural language words to schema elements that yield minimal semantic meaning.

\paragraph{\textbf{Schema Naming Conventions}}

The majority of database schema naming best practices originate from \emph{practitioners} and are generally published as software documentation, organization policies, tutorials, etc. 
We find that there is a gap in database and data integration \emph{academic} literature evaluating schema identifier naming practices for any purpose.
While the semantics of schema identifiers may not have been considered as a necessary subject of database research in the past, the increasing integration of natural language interfaces to databases has elevated its importance.

Naming conventions for database schema identifiers vary by organization, database vendor, application, and purpose.
A web search for database table and column naming guidelines yields multiple resources ranging from blog posts~\cite{csharp-naming-convention}, 
StackOverflow responses~\cite{stackoverflow-naming-convention}, DBMS vendor documentation~\cite{oracle-schema-naming-guidelines}, and tutorials~\cite{inbo-naming-convention}.
Poor schema identifier naming practices is considered a database code smell~\cite{10.1145/3183519.3183529} where meaningless identifier names should be avoided.
Generally, the most consistent best practices include selecting descriptive and concise names that contain only commonly-understood abbreviations and acronyms, though some conventions suggest the use of abbreviated prefix and suffix modifiers that describe application associations, or entity purpose~\cite{oracle-schema-naming-guidelines-2}.

In our research, we identified several databases containing schemas with varying levels of human-readability and understandability (what we will call naturalness) which suggests that there can be a tendency for database schema designers to choose shorter and less descriptive identifier naming conventions.
As we will see, such naming shortcuts can negatively affect NL-to-SQL performance.

\subsection{Our Benchmark Artifacts and Analyses}
Given the above context of our benchmarking setup, we now explain the new artifacts in SNAILS, followed by a summary of our empirical analysis.

\paragraph{Artifact 1: Real-World Database Schemas}
The SNAILS benchmark contains several new \emph{real-world database schemas} that are not part of existing NL-to-SQL benchmarks (Artifact 1).
Our focus on schema naming motivates the creation of a new novel benchmark dataset, because existing benchmark naturalness levels are higher than those of many real-world schemas, and other real-world schema collections including SchemaPile~\cite{doehmen2024schemapile} lack the necessary database instances to enable NL-to-SQL evaluation.
In our analysis of these real-world schemas, we discover that identifier naming variances generally appear in the form of abbreviations and expansions; we refer to these variances as identifier \emph{naturalness}.

\paragraph{Artifact 2: Identifier Naturalness Classifications}
Our analysis reveals that naturalness can be formalized categorically with the help of finetuned language models and feature engineering.
We then hand-label the schema identifiers, with some ML assistance, to classify their naturalness level and produce a new golden labeled dataset.
We classify identifiers into one of 3 naturalness levels (Regular, Low, and Least) (Artifact 2).
This dataset, consisting of over 17,000 labeled identifiers, serves as the training data for the naturalness classifiers described next.

\paragraph{Artifact 3: Naturalness Classifiers}
We experiment with various classification approaches, and make available the models trained to classify the naturalness of a database schema identifier (Artifact 3).

\paragraph{Artifact 4: Naturalness-Modified Identifiers}
To better understand the effect of schema identifier naturalness, and to enable within-database experiments, we create alternate versions of each real-world schema identifier at each naturalness level (Artifact 4).
This dataset serves two purposes: 1) Training data for ML-based naturalness modifiers, and 2) Generation of schemas with varying naturalness levels to analyze the impact of naturalness on NL-to-SQL performance.
We modify the identifiers with the assistance of LLM prompting, finetuned models, and database metadata.

\paragraph{Artifact 5: Naturalness Modifier}
We offer an in-context learning-based prompting strategy for identifier naturalness reduction (or abbreviation).
We also provide an identifier naturalness increaser (or expander) that leverages retrieval augmented generation, interactive few-shot example building, and database metadata parsing methods to streamline the database naturalness improvement process.

\paragraph{Artifact 6: NL-to-SQL Question Query Pairs}

The SNAILS benchmark contains 503 NL question-SQL query pairs which we use for NL-to-SQL performance analysis of 5 LLMs.
We created this new collection as another hand-labeled golden dataset without the use of AI-based workflows (Artifact 6).

\paragraph{\textbf{Experimental Evaluation}}

Using the SNAILS benchmark artifacts, we analyze and experiment with the effects of schema identifier naturalness on LLM NL-to-SQL performance.
We select 5 publicly-available LLMs: OpenAI's GPT-3.5, GPT-4o, a finetuned variant of Meta's Code-Llama, Google's newest Gemini 1.5, and CodeS finetuned for NL-to-SQL. We evaluate them using both execution result set matching and a novel identifier set comparison approach that pinpoints schema linking performance.

In this paper we focus primarily on a simple zero-shot prompting of the LLM for our experiments.
We recognize that this may not be the best for overall execution accuracy, but it helps us isolate the impact of schema identifier naturalness in this first work on this problem.
As such, more complex workflows will create confounding effects while not necessarily providing more insights into schema linking performance.
However, for completeness sake, we also compare two illustrative complex workflows: DIN SQL for task-specific prompt chaining~\cite{pourreza2023dinsql}, and CodeS~\cite{10.1145/3654930} for NL-to-SQL finetuning.

We find that schema identifier naturalness by and large does have a meaningful effect on NL-to-SQL accuracy and schema linking performance.
Specifically, identifier naturalness is moderately and positively correlated with both schema linking and execution accuracy.
Identifiers of low naturalness yield lower performing NL-to-SQL inferences in terms of both schema linking (identifier recall) and execution accuracy.
These findings have implications for practitioners who are either designing new databases intended for LLM-based applications, or seeking to augment existing RDBMSs with an LLM-based NL-to-SQL interface.

In summary, this paper makes the following contributions:
\begin{itemize}
  \item We propose a novel measure of \emph{naturalness} of a database schema identifier and demonstrate through extensive experiments that naturalness has a significant effect on LLM schema linking performance in the context of NL-to-SQL.
  \item We provide a hybrid LLM-generated and human-curated training dataset (Artifact 2) and language model (Artifact 3) for schema naturalness classification.
  \item We offer a new multi-domain NL-to-SQL evaluation benchmark collection consisting of 9 real-world relational databases (Artifact 1) and 503 unpublished NL-to-SQL query pairs (Artifact 6) that do not exist in any LLM training corpora.
  \item We create a novel labeled dataset of alternate naturalness levels that map the identifiers from Artifact 1 to hybrid LLM-human curated identifiers of different naturalness levels (Artifact 4), and methods for expanding and abbreviating identifiers to change their naturalness (Artifact 5).
  \item We conduct an extensive empirical analysis of the performance of 5 popular foundational LLMs over our benchmark using a novel schema linking metric for NL-to-SQL.
  \item We propose a realistic workflow that enables the preservation of existing database integrations while offering LLM-based NLIs a natural view of a target schema.
\end{itemize}



\section{Schema Identifier Naturalness}


\begin{table}[t]
  \centering
  \caption{Example identifiers and their naturalness levels, from the SNAILS naturalness labeled dataset (Artifact 2).}
  \begin{tabular}{p{4.5cm}p{3.1cm}p{3.1cm}}
  \toprule
  \textbf{Regular} & \textbf{Low} & \textbf{Least}\\
  \midrule
  airbag & AccountChk & AdCtTxIRWT \\
  AdaptiveCruiseControl & IsueFrDate & COGM\_Act \\
  ModelYear & RecvAsst & DfltSlp \\
  service\_name & UsrQuery & FNDAbs \\
  Research\_Staff & ValueOfT & CSI22 \\
  \bottomrule
  \end{tabular}
  \label{table:nat-cat-examples}
\end{table}

Intuitively, naturalness can be thought of as the degree to which a phrase, or word, resembles natural language. 
Naturalness is a concept and target of research in field of controlled natural languages~\cite{10.1162/COLI_a_00168}, where controlled language syntax is evaluated in terms of naturalness levels. 
Recent NL-to-SQL research also defines and measures naturalness~\cite{10.14778/3494124.3494139} for the purpose of evaluating the naturalness of natural language question utterances, but avoids measuring the naturalness of schema elements.

To the best of our knowledge, no prior attempts have been made to definitively measure the naturalness of a database schema's identifiers.
In order to achieve this goal, we propose a three-category naturalness classification scheme in order to measure the effects of naturalness on NL-to-SQL performance.

\subsection{Naturalness Categories}
\label{section:naturalness-categories}

\begin{figure}
  \centering
  \includegraphics[width=\figwidthmod\linewidth]{figures/mean_token_in_dictionary.pdf}
  \caption{\emph{Mean Token in Dictionary}, the proportion of tokens in an identifier that match a word in an English dictionary, generally aligns with the SNAILS 3-class naturalness categorization approach.}
  \label{fig:meantokenindictionary}
\end{figure}

As the first work on this topic of how schema identifier naturalness affects LLMs, we seek to define a preliminary metric--one that is consistent and descriptive enough to differentiate between naturalness levels and to measure their effects.

To gain insights into naturalness-related trends in the SNAILS datasets, we create a \emph{mean token-in-dictionary} measurement that describes the proportion of tokens in an identifier that exactly match a word in a comprehensive English word list.
Figure \ref{fig:meantokenindictionary} reveals differences between each naturalness category where Least naturalness identifiers contain fewer in-dictionary tokens, and Regular naturalness identifiers are more likely to consist of in-dictionary tokens.
This distribution suggests that because the bulk of the training corpora of LLMs is human-generated natural language text, what humans consider ``natural'' for such identifiers generally aligns with how LLMs react to them.

Examples of schema identifiers and their naturalness categories are displayed in Table \ref{table:nat-cat-examples}.
We define these categories with the underlying assumption that the identifiers are named as some semantic representation of the data, and that naming-related problems of interest are related how an identifier is codified.
That is, identifiers are assumed to not be random character sequences or random words that do not correspond to the content of the database entities they represent.
With this assumption in mind, we categorize naturalness into 3 discrete levels as follows: 
\begin{itemize}
  \item \textbf{Regular}: The identifier contains complete English words with no abbreviations or acronyms, or contains only acronyms in common usage (e.g., ID or GPS).
  \item \textbf{Low}: The identifier contains abbreviated English words and less common acronyms that are usually recognizable by non-domain experts (e.g., UTM or  CPI). The meaning of the identifier can be inferred without consulting external documentation.
  \item \textbf{Least}: The identifier's meaning cannot be inferred by non-experts due to indecipherable acronyms or abbreviations, and external metadata or other documentation must be consulted in order to determine its purpose.
\end{itemize}

While we recognize that naturalness can also be treated as a continuous spectrum, between the choices of continuous scoring and discrete categories, we select the latter as an initial approach to naturalness evaluation.
The primary factors underlying this choice are the level of effort required to conduct human-based scoring of a large set of database identifiers, and the difficulty of consistently scoring naturalness on a continuous range over a large set of data.
Therefore, we use an intuitive and easily-verifiable discrete 3-class taxonomy in the first work on this topic.


\subsection{Naturalness Classification}
\label{subsection:naturalness-scoring}

To consider naturalness as a factor in NL-to-SQL performance, we derive naturalness scores of the target schemas' identifiers.
We use this score to consider effects of individual identifier naturalness, schema naturalness, and query identifier naturalness.
Because manual naturalness classification can be a time consuming task for large schemas, we automate the process by training a machine learning-based classifier.
This effort is beneficial in multiple situations.
First, it can ease some manual effort of the labeling process and make the process of scaling to more databases in the future less labor intensive.
Second, it can help practitioners efficiently and consistently evaluate the naturalness of their own database schema identifiers prior to NLI integration.


To train a classifier to perform identifier naturalness scoring, we employ the 3-class set of naturalness categories described in Section \ref{section:naturalness-categories}, and a list of database identifiers drawn from the SNAILS real-world database schemas (Artifact 1).
We categorize the naturalness of each identifier to generate the SNAILS \emph{identifier naturalness classification} labeled data (Artifact 2) which we use for ML-based naturalness classifier training, evaluation and testing.

We evaluate multiple classification approaches including heuristic-based word matching, few-shot LLM prompting with GPT-3.5 and GPT-4, and LLM finetuning.
The GPT-4 few-shot approach achieves 74 percent accuracy and an f1 score of 0.77.
We experiment with multiple finetuning collections, first using a hand-labeled collection of 1,648 naturalness classifications and then leveraging the initial classifier along with weak supervision to generate a larger collection of 17,226 labeled identifiers.
Finetuning using the second collection outperforms all few-shot approaches, with the
two best-performing classifiers fine-tuned GPT 3.5 and BERT-based CANINE~\cite{Clark-2022} models performing at 89 percent accuracy, and 0.89 f1 score.

\begin{figure}
  \centering
  \includegraphics[width=\figwidthmod\linewidth]{figures/benchmark_naturalness_compare.pdf}
  \caption{Comparison of the SNAILS database collection (Artifact 1) described in Section \ref{subsection:benchmark-datasets} to other real-world and benchmark schema collections. SNAILS naturalness proportions are generally biased toward less natural identifiers and is more consistent with the real-world SchemaPile collection than other existing benchmarks including Spider and Spider Realistic.}
  \label{fig:naturalnesscompare}
\end{figure}



Figure \ref{fig:naturalnesscompare} provides a visual comparison between the SNAILS schema collection and common NL-to-SQL benchmarks including Spider, Spider Realistic, and BIRD.
Additionally, we compare the SNAILS collection to the real-world SchemaPile collection and find that SNAILS collection proportions generally align to SchemaPile naturalness, more so than other existing benchmarks, which creates a more realistic and challenging benchmark in terms of schema naturalness.

To better understand the magnitude of naming practices in real-world schemas,
we use the CANINE-based classifier to classify the naturalness of the SchemaPile collection: a large volume of real-world database schemas~\cite{doehmen2024schemapile} that contains over 22,000 database schemas, 198,000 tables, and 1 million columns.
We find that in over 7,500 schemas (32 percent of the collection) Least natural identifiers make up at least 10 percent of the schema identifier names.
Additionally, over 5,000 schemas register a combined naturalness of 0.7 or below--an indicator that the schema contains a high level of Low and Least naturalness identifiers.
We examined the naturalness category distribution for these 5,000 schemas, and found that for this subset of schemas Low and Least naturalness identifiers outnumber Regular naturalness identifiers.
These findings reinforce the importance of the naturalness problem by confirming that, although a reasonable majority of schemas are already natural, there still exist many schemas with lower naturalness levels in the real-world--enough to motivate the formalization of schema naming quality measures.


\subsection{Identifier Schema Naturalness Mapping}
\label{subsection:naturalnessmapping}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/classify-modify-pipeline.pdf}
    \caption{Schema identifiers are classified (Artifact 2) and modified to increase or decrease naturalness as appropriate. Modified identifiers comprise the schema crosswalks used for schema modification during NL-to-SQL experimentation (Artifact 3).}
    \label{fig:classify-modify-pipeline-main}
\end{figure}

In addition to measuring the effects of identifier naturalness in existing schemas, we also seek to evaluate the effects of modifying schema naturalness.
For this purpose, we create Artifact 4, naturalness-modified identifiers.
This artifact enables schema modification during prompt generation and query inference, which provides a within-schema assessment of naturalness level effects on NL-to-SQL accuracy.

\paragraph{\textbf{Identifier Mapping}}
In addition to the ground truth, or Native, naturalness of the 9 schemas in the SNAILS real-world database collection, the naturalness-modified identifier collection contains 3 additional sets of identifiers: Regular, Low, and Least.
That is, each native identifier is mapped to 2 additional, semantically equivalent, identifiers of higher or lower naturalness, and mapped to itself for its own naturalness level (i.e., we do not generate new identifiers of the same naturalness as its native form).



Figure \ref{fig:classify-modify-pipeline-main} provides a visual example of the Native identifier \emph{VegHeight} which is classified as Low naturalness.
With this naturalness classification as a starting point, we abbreviate it further to generate a corresponding Least naturalness identifier \emph{VgHt}.
We also expand it to generate the corresponding Regular naturalness version \emph{vegetation\_height}.
We map the Native \emph{VegHeight} identifier to itself in the Low naturalness category.

\paragraph{\textbf{Naturalness Modification}}
For \emph{more natural to less natural} modifications (the abbreviator in Figure \ref{fig:classify-modify-pipeline-main}), we employ in-context learning (few-shot) prompt strategies with GPT-3.5 turbo to generate naturalness-modified identifiers (e.g., Regular to Low, Low to Least, and Regular to Least).
We favor this approach over model finetuning, as simple instructions to abbreviate the identifier coupled with several examples prove more effective and less prone to poor results (e.g., presence of unwanted characters in the modified identifier).

Automating the reverse \emph{less natural to more natural} naturalness modification (the expander in Figure \ref{fig:classify-modify-pipeline-main}) requires additional context and external knowledge from data description sources.
Though a recent project describes a promising identifier expansion strategy~\cite{nameguess} without external knowledge, it requires finetuning over a large dataset, and is likely susceptible to overfitting; therefore we opt for our own approach that incorporates the use of an LLM augmented with schema metadata lookup capability.
To accomplish this, we create a Python program with GPT interaction that takes as input metadata describing a schema's native identifiers, and outputs an identifier with regular naturalness.

\subsection{Heuristics-based scoring} 

Prior to experimenting with ML classifiers, we used a set of heuristics to score the naturalness of each identifier. 
Comparisons between the heuristics-based scoring approach and ML classification reveals that ML is superior in terms of recall, precision, and F1.
We include a description of the heuristics here for completeness, but exclude the performance results from the main body of the report.

\begin{itemize}
  \item Vectorize an English word vocabulary as frequency counts of letters in the word.
  \item With a given database identifier, vectorize the identifier as frequency counts of letters in the identifier and downsample to the English word vocabulary to words that have a superset of the letters in the identifier.
  \item Further downsample the candidate words to words where the letters appear in the same order as the words in the identifier.
  \item For each word in the downsampled vocabulary, compute the Levenshtein distance between the word and the identifier. This number is called the edit distance.
  \item For each word, count the number of possible word candidates within 1 and 2 Levenshtein distance from the word. We call this number candidate ambiguity.
  \item The distribution of candidate ambiguity across our vocabulary is highly skewed, so we take the log of the candidate ambiguity to normalize the distribution.
  \item We then calculate the naturalness score as the weighted mean of the inverse of the edit distance and the inverse of the log of the candidate ambiguity. This yields values ranging from 0 to 1, where 0 is least natural and 1 is most natural.
\end{itemize}


\subsection{Dataset Naturalness Classifications}
\label{section:natclassification}

Identifier naturalness within each dataset is categorized using the $N1$ (Regular), $N2$ (Low), and $N3$ (Least) categories.
Naturalness of table and column identifiers are cataloged both separately, and in consolidated form (i.e. tables and columns together).
Additionally, we calculate a combined naturalness score for consolidated identifiers using category weights.
\begin{equation}
  $$CombinedNaturalness = 1.0*Regular + 0.5*Low + 0.0*Least$$ 
  \label{equation:combined-naturalness}
\end{equation}
where $Regular$, $Low$, and $Least$ are proportions of schema identifiers in each respective category within the total count of identifiers in the identifier's source schema.

\subsection{Training Data Collections}

For finetuning tasks, we train language models using database identifiers extracted from the schemas in the SNAILS real-world database collection.
We begin with a human-classified collection (Collection 1), then we employ classifier models trained on Collection 1 to generate a larger set (Collection 2) of machine-classified and human-curated identifier classifications.

\textbf{Collection 1} The full dataset contains 1,648 manually classified unique schema identifiers. The identifiers are hand labeled as one of 3 naturalness levels (Regular, Low, Least). We randomly divide the data into a training, validation, and test set. This resulted in a distribution of 959 identifiers used for training, 356 for validation, and 333 for testing. 

\textbf{Collection 2} The labeled data set contains 13,722 distinct column identifiers and 3,504 distinct table identifiers for a total size n = 17,226. 
We employ GPT's Davinci model finetuned on Collection 1 to generate the preliminary naturalness scores.
The authors reviewed, and where necessary, modified the model-generated identifier scores to affirm the accuracy of the naturalness classifications.
Of the 17,223 identifiers reviewed, 15,527 naturalness scores, or 90.1 percent, were accurately predicted by the Davinci-based model.
We manually scored the incorrectly-predicted 1,696 identifiers.
For model finetuning, we randomly split the resulting data into training (n = 10,327), validation (n = 3,457), and test (n = 3,457).

\subsection{ML Classifier-based scoring}

% Table template generated by GPT3.5 and modified by GPT4
\begin{table}[t]
  \centering
  \caption{Performance comparison of different language models for classifying a database identifier's naturalness}
  \begin{tabular}{p{4cm}|cccc}
  \toprule
  \textbf{Model} & \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} \\
  \midrule
  GPT-3.5-FewShot & 0.646 & 0.623 & 0.638 & 0.630 \\
  CANINE-Seq C1 & 0.719 & 0.699 & 0.727 & 0.712 \\
  GPT-4-FewShot & 0.742 & 0.742 & 0.792 & 0.766 \\
  CANINE-Seq+TG C1 & 0.829 & 0.829 & 0.838 & 0.833 \\
  GPT-3.5-FineTune & \textbf{0.899} & 0.878 & 0.877 & 0.878 \\
  GPT-3.5-FineTune+TG & 0.896 & \textbf{0.896} & 0.897 & 0.896 \\
  CANINE-Seq+TG C2 & 0.896 & \textbf{0.896} & \textbf{0.898} & \textbf{0.897} \\
  \bottomrule
  \end{tabular}
  \label{table:classifierperformance}
\end{table}

The use of pre-trained language models is a SoTA approach for classification problems~\cite{10.1145/3605943}, and we experiment with various attempts at model-based scoring, including few-shot learning via the GPT API, and finetuning several BERT-like language models on our dataset of database identifiers to create a second larger collection of identifier naturalness scores. 
Since the presence of acronyms and abbreviations is a significant determining factor of identifier naturalness, a primary consideration for our naturalness scoring task is the granularity of the tokenizer output.
For this reason, we use models that employ either character-level tokenization, word part tokenization, or byte pair tokenization techniques.
We select 2 approaches: 1) Use of a foundational LLM in various capacities; and 2) Finetuning of a character-level token language model.

\subsection{Character Tagging Feature}
There are intuitive structural differences between abbreviated words and their complete counterparts.
Specifically, we observe that word abbreviations generally contain more consonants than vowels, as vowels seem more likely to be removed during abbreviation.
For this reason, we include a pre-processing step that generates a sequence of special characters that correspond to the type of characters of the identifier to be classified. 
The sequence is then concatenated with the identifier and passed to the language models during training and inference.
We refer to this approach as \emph{character tagging}, and models employing tagging are labeled with TG in Table \ref{table:classifierperformance}.
We concatenate the tag sequence to the identifier and pass it to the language models for training and inference. Special characters include:
\begin{itemize}
  \item \textbf{\textasciicircum}: Vowels
  \item \textbf{+}: Consonants
  \item \textbf{\#}: Numbers
  \item \textbf{\$}: Special characters
  \item \textbf{$\ast$}: Any character not in the above categories
\end{itemize} 

For example, the identifier \texttt{AuthorID\_5} would be pre-processed as follows:

\begin{verbatim}
    AuthorID_5 ^^++^+^+$#
\end{verbatim}

Both GPT- and CANINE-based models exhibit improvement in F1 scores when using the character tagging feature.

\subsection{GPT 3.5/4 Turbo Few-Shot-Based Scoring}

\begin{figure}
  \begin{Verbatim}[frame=single,fillcolor=\color{lightgray}]
The following is a list of database identifiers and 
labels that indicate how closely they resemble 
natural english words:
N1: most natural english words
N2: second most natural english words 
(e.g. abbreviations or combinations of 
natural words and acronyms)
N3: third most natural english words 
(e.g. very short abbreviations with obscured 
meaning or acronyms)

identifier:	CASENO                      Label:	N1
identifier:	BENTHOS_TotalAreaSampled_m2 Label:	N2
identifier:	CAUSE3                      Label:	N1
identifier:	MT_RIVPACS_2011_OTU         Label:	N3
identifier:	ACTIVATE                    Label:	N1
identifier:	MotorcycleChassisTypeId     Label:	N1
identifier:	First_Name                  Label:	N1
identifier:	IPCAREA_2ND                 Label:	N2
identifier:	INJNO                       Label:	N2
identifier:	tbl_MicroHabitat            Label:	N2
identifier:	EMSGCSEYE                   Label:	N3
identifier:	HEADRESTDAM                 Label:	N2
identifier:	AutoPedestrianAlertingSound Label:	N1
identifier:	ModelTest                   Label:	N1
identifier:	tlu_topo_position           Label:	N2
identifier:	Understory_Comp             Label:	N1
identifier:	BAGDAMAGE                   Label:	N1
identifier:	HARNESSDESIGN               Label:	N1
identifier:	Coord_Syst                  Label:	N2
identifier:	CINJSEV                     Label:	N2
identifier:	JKWGT12                     Label:	N3
identifier:	_IDENTIFIER_                Label:
  \end{Verbatim}
\caption{Few-Shot naturalness classification completion prompt template}
\label{fig:fewshotclassifierprompt}
\end{figure}

We experiment with the effectiveness of few-shot prompting to classify identifier naturalness.
We opt to provide one set of instructions followed by a series of 25 randomly selected human-validated examples of naturalness levels.
Figure~\ref{fig:fewshotclassifierprompt} is an example of such a randomly generated few-shot prompt templates.
We perform text replacement on the trailing row by replacing the \_IDENTIFIER\_ text with the identifier to be classified. 
This approach does not require model pre-training, but this convenience is paid for in terms of the number of tokens in the prompt, and classifying a large schema with this method can incur very high LLM usage costs.




\subsection{GPT Davinci Finetuning}

We train the Davinci-based completion models using the OpenAI command line API.
We generated models with character tagging, as well as models without tagging.
Below is an excerpt from the tagging-based training data.

\begin{figure}
\begin{Verbatim}[frame=single,fillcolor=\color{lightgray}]
{"prompt":"ADDRESS ^+++^++ ->","completion":" N1"}
{"prompt":"AIS ^^+ ->","completion":" N3"}
{"prompt":"AISCODE ^^++^+^ ->","completion":" N3"}
{"prompt":"BACKBPILL +^++++^++ ->","completion":" N2"}
{"prompt":"ALIGNMENT ^+^+++^++ ->","completion":" N1"}
{"prompt":"ARRMEDICAL ^+++^+^+^+ ->","completion":" N2"}
\end{Verbatim}
\caption{GPT Davinci finetuning example data with tagging feature.}
\end{figure}


Inference using charager tagged models requires appending the tag to the identifier in the same format as the training data.

\subsection{CANINE Fine Tuning}
CANINE~\cite{Clark-2022} is a BERT-based language model that tokenizes inputs at the character level. 
We trained a sequence classification head using both generation 1 and generation 2 data sets using a single NVIDIA GTX 1080 GPU. 
We employed the HuggingFace Transformers library, CUDA 12.1, and Torch 2.0.1 to fine tune the 'google/canine-s' model.
Hyperparameter tuning was conducted using the optuna library, which resulted in the parameter settings:
\begin{itemize}
  \item Optimizer: adamw hf 
  \item Learning rate: 4.910828967396573e-05
  \item Per device training batch size: 24
  \item Per device evaluation batch size: 12
  \item Number of epochs: 15
  \item Weight decay: 0.04168784348465411
\end{itemize}

Models were trained both with, and without, the character tagging feature.
We offer the evaluated models in our project repository.
The \emph{snails\_naturalness\_classifier.py} Python file contains the \emph{CanineIdentifierClassifier} class.
This class provides a simple \emph{classify\_identifier} method for using the CANINE model to classify identifier naturalness with or without character tagging.

\subsection{Tokenizers}

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.5\linewidth]{figures/identifier-char-count-cdf.pdf}
  \caption{Cumulative distribution of schema identifier character counts by naturalness level. More natural (less abbreviated) identifiers logically have more characters.}
  \label{fig:charcountcdf}
\end{figure}

\begin{figure*}[ht]
  \centering
  \includegraphics[width=\textwidth]{figures/identifier-token-count-cdf.pdf}
  \caption{Token count CDF, by naturalness level, for each language model.}
  \label{fig:tokencountcdf}
\end{figure*}

\begin{figure*}[ht]
  \centering
  \begin{subfigure}{\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/identifier-token-char-ratio-cdf.pdf}
  \end{subfigure}
  \caption{Token counts to character count ratio, by naturalness level, for each language model. More natural identifiers generally contain fewer tokens-per-character than less natural identifiers, suggesting a higher presence of in-vocabulary keywords for more natural identifiers.}
  \label{fig:tokencdfs}
\end{figure*}

We examine the relationship between tokenization and naturalness by generating token counts, character counts, and a character-to-token ratio of each identifier.
As expected, due to the unabbreviated nature of the identifiers, more natural identifiers have more characters (see Figure \ref{fig:charcountcdf}). 
Perhaps more surprising, token count is \emph{not} very sensitive to naturalness levels, mainly due to the general behavior where more abbreviated identifiers will have character sequences not found in the LLM tokenizer's vocabulary.
When a character sequence is not present in the vocabulary, the tokenizer will split the sequence into multiple subtokens.

\begin{equation}
  \label{eq:ttcr}
  TCR = \frac{|I_{tokens}|}{|I_{characters}|}
\end{equation}

Because there is not a clear relationship between token count and naturalness, we derive a token-to-character ratio metric (Equation \ref{eq:ttcr}) which is the count of identifier tokens $I_{tokens}$ divided by the count of identifier characters $I_{characters}$.
What we see in Figure \ref{fig:tokencdfs} is a fairly strong differentiation between naturalness levels and TCR, where more natural identifiers have lower TCR than less natural identifiers.
We believe that this hints at the effects of in- vs. out-of-vocabulary character sequences and the strength of their semantic meaning in latent space.
However, the relationship is not so strong that TCR alone can serve as a useful classification method for identifier naturalness.
We leave additional exploration of this topic to future research.



\section{Naturalness-Modified Identifiers}


Naturalness modification is the process of changing an identifier in such a way that it assumes a naturalness category that is not its original classification.
Modifying an identifier to become less natural is useful for creating benchmark schemas of varying naturalness levels.
The same benefit applies to the process of modifying less natural identifiers to become more natural; this direction of modification also generally yields improved NL-to-SQL performance, as is demonstrated in the experiment and evaluation sections of this report.

Naturalness-modified identifiers generated by the ML-based approaches described next are human-validated and, when necessary, modified.
Once validated, the identifiers are added to our ground truth dataset and used for the prompt and query naturalness modification processes described elsewhere in this report.

\subsection{Decreasing Naturalness (Abbreviation)}
Decreasing naturalness generally involves the removal of characters from an identifier in a manner that shortens the length while retaining some structure that still allows for some measure of readability.
This cannot be achieved by randomly removing characters from an identifier, so we elect to experiment with both machine learning-based and LLM-based approaches to decrease identifier naturalness.
As with our classification approaches, we experiment with both engineered few shot prompts targeted at a general purpose foundational LLM (GPT), and a fine tuning approach (GPT Davinci).

\paragraph{\textbf{FPT Davinci Fine Tuning Abbreviation}}

Seperate models are trained for converstion tasks from one naturalness level to a model-specific alternative naturalness level.
This resulted in the following fine tuned models:

\begin{itemize}
  \item Regular to Low
  \item Regular to Least
  \item Low to Least
\end{itemize}

Each fine tune dataset consists of 176 randomly selected identifiers and human-created naturalness modifications.
Figure~\ref{fig:regtoleasttrainingdata} provides an example of Regular to Least model training data:

\begin{figure}
\begin{Verbatim}[frame=single,fillcolor=\color{lightgray}]
{"prompt":"Plot ->","completion":" p\n"}
{"prompt":"Metals ->","completion":" mt\n"}
{"prompt":"Station_ID ->","completion":" S_ID\n"}
{"prompt":"FUELEAK ->","completion":" F_Lk\n"}
\end{Verbatim} 
\caption{Regular-to-Least naturalness modification training data example.}
\label{fig:regtoleasttrainingdata}
\end{figure}

The outputs of these finetuned models require significant adjustment by human researchers; so we elect to employ an alternative approach described next.

\paragraph{\textbf{GPT Few Shot Abbreviation}}

GPT 3.5-based few shot prompting (see example below) resulted in the most consistent outputs, with a reasonably low prompt token count.
Rather than explaining the different categories followed by an instruction to convert an identifier to a specific category, we find that providing a simple instruction to \emph{abbreviate the database schema identifier to make it slightly shorter} followed by several examples is more effective.
Figure~\ref{fig:regtolowprompt} contains an example prompt instructing an LLM to abbreviate an identifier from Regular to Low naturalness.

\begin{figure}
\begin{Verbatim}[frame=single,fillcolor=\color{lightgray}]
Abbreviate the database schema identifier 
to make it slightly shorter:
Protocol_Name -> Protcl_Nm

Abbreviate the database schema 
identifier to make it slightly shorter:
WaterTemperature -> WaterTemp

Abbreviate the database schema
identifier to make it slightly shorter:
Customer -> Custmr

Abbreviate the database schema 
identifier to make it slightly shorter:
_IDENTIFIER_ ->
\end{Verbatim}
\caption{Regular-to-Low naturalness modification LLM prompt template.}
\label{fig:regtolowprompt}
\end{figure}

\subsection{Increasing Naturalness (Expansion)}

Increasing naturalness requires the expansion of an abbreviated identifier.
This is a challenging task for pretrained language models that lack a context grounded on the specifics of the data to be expanded.
A recent attempt at performing abbreviation expansion with model fine tuning~\cite{nameguess}  appears to be a promising direction for future research, but is too complex and resource intensive to extend to the SNAILS artifact datasets. Thus, we elect to enrich our process with external database metadata via retrieval augmented generation (RAG).

\paragraph{\textbf{Expansion Process}}
In order to accomplish the RAG-based expansion process, we engineer a solution that employs a database metadata reader capable of reading .pdf, .xml, and .csv formatted metadata.
Metadata is read and indexed at the word level, where an array of file locations (page and line numbers for pdf, line numbers for xml and csv) where words occur are mapped to each word.
When a user keys in an identifier to modify, file locations where the identifier exists in the document are returned via index lookup.
These index locations are used as the centerpoints of context windows that retrieve the surrounding content.
This content is added to a fewshot prompt to provide the language model with document content that is likely to contain references to, and definitions of, the provided identifier.

The fewshot prompt for generating an expanded identifier adheres to the template displayed in figure~\ref{fig:ragtemplate}.

\begin{figure}
\begin{Verbatim}[frame=single,fillcolor=\color{lightgray}]
Using the following text extracted from a 
data dictionary:
__CONTEXT__

In the response, provide only the old identifier 
and new identifier (e.g. "old_identifier, 
new_identifier").
Create a meaningful and concise database identifier 
using SQL compatible complete words to represent 
abbreviations and acronyms for only 
the identifier __IDENTIFIER__:
\end{Verbatim}
\caption{Retrieval augmented generation identifier expansion prompt template.}
\label{fig:ragtemplate}
\end{figure}

The \_\_CONTEXT\_\_ placeholder is replaced with up to ten context window-length excerpts from the database metadata.
Figure~\ref{fig:ragfsprompt} is an example of a completed prompt using the NYSED .pdf based data manual with context window of 200 characters:

\begin{figure}
\begin{Verbatim}[frame=single,fillcolor=\color{lightgray}]
Using the following text extracted from a data dictionary:
r school  Text  255
YEAR  Reporting Year (2021 for 2020 -21; 2022 for 2021 -22) 
Number  4
NUM_TEACH  Number of teachers as reported 
in the Student Information Repository System
(SIRS)  Number  12
NUM_TEACH_INEXP  Number of teachers with fewer 
than four years of experience in their positions  
Number  12
PER_TEACH_INEXP  Percent of teachers with 
fewer than four years of experience in their posi

In the response, provide only the old identifier 
and new identifier (e.g. "old_identifier, new_identifier").
Create a meaningful and concise database identifier 
using SQL compatible complete words to represent 
abbreviations and acronyms for 
only the identifier num_teach_inexp:

num_teach_inexp, number_of_teachers_inexperienced
\end{Verbatim}
\caption{Retrieval augmented generation identifier expansion few-shot prompt example.}
\label{fig:ragfsprompt}
\end{figure}

In this successful example, we see that the identifier \emph{num\_teach\_inexp} has been expanded to a more natural \emph{number\_of\_teachers\_inexperienced}.
This is despite the observation that the data retrieved from the .pdf file is quite unstructured and contains document artifacts.
A sufficiently wide context window coupled with the retrieval of multiple occurences of the identifier in the document generally results in valid expansions.

\paragraph{\textbf{Prompt Building}}
In order to generate few shot prompts over an arbitrary metadata source, some prompt engineering is necessary.
Generally, hand-crafted prompt building is suitable approach; but it does not scale nor does it lend itself to an automated solution that can be deployed beyond a research lab.
To make this process more portable, we introduce a command line-based subroutine that enables the automatic build of a five example few shot prompt.
In this process:
\begin{enumerate}
  \item User enters an identifier
  \item Zero shot prompt -> expanded identifier
  \item User reviews and validates identifier
  \item Correct: identifier added to example list
  \item Incorrect: User tries again with different identifier
  \item User enters another identifier
  \item Few shot prompt (with prior successes as examples) -> expanded identifier
  \item Correct: identifier added to example list
  \item Incorrect: User tries again with different identifier
  \item Process repeats until five successful examples are generated
\end{enumerate}

Once a fewshot prompt has been created for a given database's metadata, the prompt is stored for any future program runs.
This particular aspect of our project was built to support our research efforts, and we did not perform any experiments to evaluate its overall accuracy and usability.
We leave these tasks as potential future research opportunities.



\section{Base Collections}


Given the recency of the LLMs selected for evaluation in this project, and the relative maturity of existing NL-to-SQL benchmarks, we believe that foundational LLMs have been exposed to existing benchmark training and development NL questions and queries in their training corpora.
NL-to-SQL performance differences between queries over seen vs. unseen schema are significant~\cite{49288}, and we seek to avoid as much bias as possible due to intentional or unintentional pre-training on existing benchmark datasets.

We also find that existing benchmarks including Spider~\cite{Yu&al.18c}, and BIRD~\cite{li2023llm}, do not match the identifier naturalness distribution of real-world schema collections such as SchemaPile~\cite{doehmen2024schemapile}.
Although SchemaPile is a very large representation of real-world schemas, it does not contain database instances necessary for benchmark performance evaluations; so, we are not able to leverage its dataset in the creation of a new benchmark.
To reduce bias due to benchmark data exposure, and to create a benchmark more representative of real-world schema naming, SNAILS contains two artifacts for NL-to-SQL benchmarking: Artifact 1, which is a collection of 9 publicly-available database schemas and data; and Artifact 6, a human-generated set of 503 NL question - gold query pairs.

\subsection{Datasets}
\label{subsection:benchmark-datasets}

\begin{table}[t]
  \centering
  \caption{SNAILS Real-World Database Schemas}
  \begin{tabular}{p{2cm}cccc}
  \toprule
  \textbf{Database} & \textbf{Tables} & \textbf{Columns} & \textbf{Questions} & \textbf{Org} \\
  \midrule
  ASIS & 36 & 245 & 40 & NPS \\
  ATBI & 28 & 192 & 40 & NPS \\
  CWO & 13 & 71 & 40 & NPS \\
  KIS & 18 & 157 & 40 & NPS \\
  NPFM & 27 & 190 & 40 & NPS \\
  NTSB & 40 & 1611 & 100 & NHTSA \\
  NYSED & 27 & 423 & 63 & NYSED \\
  PILB & 21 & 196 & 40 & NPS \\
  SBOD & 2588 & 90,477 & 100 & SAP \\
  \bottomrule
  \end{tabular}
  \label{table:benchmarkschemas}
\end{table}

\paragraph{\textbf{Native Schemas}}
The SNAILS real-world database schema collection (Artifact 1) consists of 9 databases sourced from multiple locations.
We refer to the schema identifier names as they exist in the source databases as \emph{Native}, and we classify each schemas' Native naturalness level (see Figure \ref{fig:databasenaturalness}).
Domain diversity facilitates a more thorough evaluation~\cite{finegan-dollak-etal-2018-improving}; so, SNAILS database collections span multiple domains.
Domain coverage includes scientific nature observation records, vehicle safety statistics, primary school performance data, and business resource planning.

The U.S. National Parks Service's IRMA Portal~\cite{nps-irma-portal} is the source of the scientific observation databases which include the 
Field Data for the Inventory of Amphibians and Reptiles of Assateague Island National Seashore (\textbf{ASIS})~\cite{assateague-herp}, 
Great Smoky Mountains All Taxa Biodiversity Inventory (\textbf{ATBI}) Plot Vegetation Monitoring Database~\cite{gsmnp-atbi}, 
Wildlife Observations Database: Craters of the Moon National Monument and Preserve 1921-2021 (\textbf{CWO})~\cite{craters-of-the-moon-wildlife}, 
Exotic and Invasive Plants Monitoring Database (\textbf{KIS})~\cite{klamath-inventory}, 
Northern Plains Fire Management (\textbf{NPFM})~\cite{ngp-fire}
and Pacific Island Network Landbird Monitoring Dataset (\textbf{PILB})~\cite{pilb-dataset}. 

The National Transportation Safety Bureaus 2021 safety sampling dataset~\cite{ncsa2022overview, crash-investigation-sampling-system} is the source of SNAILS \textbf{NTSB} safety statistics database. 
We source school performance data (\textbf{NYSED}) from the New York State Education Department~\cite{nysed-report-card}. 

The business resource planning database \textbf{SBOD} is a training example of the popular SAP Business One system, and is publicly available in MS SQL server backup format~\cite{sap-demo}.
The SBOD schema consists of an extremely large number of tables and columns; so pruning is required to fit the schema within the context window of the LLMs we compared.
We reduce the schema knowledge token requirements by segmenting the SBOD schema into submodules and further reducing tables through data profiling.
Additional information on the SBOD schema knowledge management is available in the \iftechreport{appendix}\else{technical report \cite{techreport}}\fi.

Each database was migrated from its source format into an MS SQL Server database.
Several databases contained identifiers with whitespace characters, which is uncommon in most schemas.
To mitigate whitespace-related inference failures as a confounder, we modify the native identifiers by replacing whitespace characters with underscore characters.
In total, 148 out of over 19,000 total identifiers (less than .01 percent) contained at least 1 whitespace character.

\paragraph{\textbf{Native Schema Naturalness Levels}}

Since the intent of this project is to measure the effect of schema naturalness, we first check if there is sufficient distribution of naturalness levels across the collection. 
We employ the GPT-3.5-based classifier described in Section \ref{subsection:naturalness-scoring} to evaluate the naturalness of the native schema identifiers.



Figure \ref{fig:databasenaturalness} displays the proportions of identifiers in each naturalness category, as well as the combined naturalness, in each native schema.
From the chart, we can see that the schemas in the SNAILS collection described in Section \ref{subsection:benchmark-datasets} represent a heterogeneous selection of naturalness combinations.

\begin{figure}
  \centering
  \includegraphics[width=\figwidthmod\linewidth]{figures/naturalness_by_db.pdf}
  \caption{Proportion of identifiers in each naturalness category within the SNAILS real-world database collection (Artifact 1). Horizontal line markers indicate calculated combined naturalness as described in Section~\ref{section:natclassification}.}
  \label{fig:databasenaturalness}
\end{figure}

\paragraph{\textbf{Modified (Virtual) Schemas}}
To control for confounding factors such as schema structure, normalization levels, and constraint variances between native schemas, we perform within-database evaluations of naturalness.
To accomplish this, we generate 3 additional \emph{virtual} schemas using the naturalness-modified identifiers (Artifact 4) described in Section \ref{subsection:naturalnessmapping}.
Each virtual schema is representative of a naturalness category, where schema identifiers are replaced with a semantically equivalent identifier of a different naturalness level.
This results in 4 schema versions per database in the base collection: Native, Regular, Low, and Least.

The modified schemas are virtual because we do not create database instances that can be queried directly.
Rather, we query virtual schemas via identifier replacement in prompts and generated queries using processes described in Section \ref{section:nl-to-sql-benchmarking-setup}.
This approach reduces storage overhead.
It also enables possible future schema variations of different naturalness proportions without the need to instantiate additional database instances.

\paragraph{\textbf{SNAILS Database Selection and Extension Processes}}
The initial 9 datasets and schemas are included because they were (1) publicly available, (2) not included in any prior NL-to-SQL benchmarks, (3) contained relational tables with dependencies and database instances with values, (4) had available table and column metadata, (5) represented a diversity of application domains, and (6) contain data potentially useful for real-world data analysis or data science applications.
Databases are not selected or pre-screened using perceived naturalness as criteria.

We view the initial 9 schemas as a starting point from which the SNAILS dataset can grow.
Researchers who wish to extend the SNAILS collection should use the same selection criteria.
In addition, the extension process must ensure that new databases:  (1) can be represented as MS SQL Server instances, (2) each native identifier's naturalness is classified according to defined criteria using the SNAILS naturalness classifier, and (3) that native identifiers are modified using the SNAILS modification artifacts to create alternate naturalness levels. 

\subsection{Data Source Details}

\subsubsection{Field Data for Assateague Island National Seashore Amphibian and Reptile Inventory (ASIS)}

\paragraph{Data Description} 
The ASIS database~\cite{assateague-herp} is sourced from the National Parks Service (NPS) Irma portal~\cite{nps-irma-portal} and contains scientific observation data of wildlife in the Assateague Island National Seashore preserve.

% \begin{figure}[H]
%   \centering
%   \includegraphics[width=\linewidth]{figures/query-stats/ASIS_20161108_HerpInv_Database-stats-clause-counts.png}
%   \caption{Gold query clause composition - ASIS database}
%   \label{fig:query-complexity-asis}
% \end{figure}

\paragraph{ASIS Database Technical Details}
\begin{itemize}
  \item Data source format: Microsoft Access
  \item Migration method: SQL Server Migration Assistant
  \item Table count: 36
  \item Column count: 245
  \item Mean columns per table: 6.125
  \item NL Questions: 40
  \item Combined naturalness level: 0.77 
\end{itemize}

\subsubsection{Great Smoky Mountains All Taxa Biodiversity Inventory (ATBI) Plot Vegetation Monitoring Database}

\paragraph{Data Description}
The ATBI database~\cite{gsmnp-atbi} contains scientific observations of vegetation within the Great Smoky Mountains national park.

% \begin{figure}[H]
%   \centering
%   \includegraphics[width=\linewidth]{figures/query-stats/ATBI-stats-clause-counts.png}
%   \caption{Gold query clause composition - ATBI database}
%   \label{fig:query-complexity-atbi}
% \end{figure}

\paragraph{ATBI Database Technical Details}
\begin{itemize}
  \item Data source format: Microsoft Access
  \item Migration method: SQL Server Migration Assistant
  \item Table count: 28
  \item Column count: 192
  \item Mean columns per table: 6.857
  \item NL Questions: 40
  \item Combined naturalness level: 0.70 
\end{itemize}

\subsubsection{Klamath Inventory and Monitoring Network (KIS)}

\paragraph{Data Description}
The Klamath Invasive Species (KIS) database~\cite{klamath-inventory} contains scientific observations of exoctic and invasive plants observed in Klamath Falls, Oregon.

% \begin{figure}[H]
%   \centering
%   \includegraphics[width=\linewidth]{figures/query-stats/KlamathInvasiveSpecies-stats-clause-counts.png}
%   \caption{Gold query clause composition - KIS database}
%   \label{fig:query-complexity-kis}
% \end{figure}

\paragraph{KIS Database Technical Details}
\begin{itemize}
  \item Data source format: Microsoft Access
  \item Migration method: SQL Server Migration Assistant
  \item Table count: 18
  \item Column count: 157
  \item Mean columns per table: 8.72
  \item NL Questions: 40
  \item Combined naturalness level: 0.79 
\end{itemize}

\subsubsection{Pacific Island Network Landbird Monitoring Dataset}

\paragraph{Data Description}
Pacific island landbirds (PILB) database~\cite{pilb-dataset} contains scientific observations of bird observations in various Pacific islands within the US states and territories.

% \begin{figure}[H]
%   \centering
%   \includegraphics[width=\linewidth]{figures/query-stats/PacificIslandLandbirds-stats-clause-counts.png}
%   \caption{Gold query clause composition - PILB database}
%   \label{fig:query-complexity-pilb}
% \end{figure}

\paragraph{PILB Database Technical Details}
\begin{itemize}
  \item Data source format: Microsoft Access
  \item Migration method: SQL Server Migration Assistant
  \item Table count: 21
  \item Column count: 196
  \item Mean columns per table: 9.33
  \item NL Questions: 40
  \item Combined naturalness level: 0.75 
\end{itemize}

\subsubsection{Wildlife Observations Database: Craters of the Moon National Monument and Preserve 1921-2021}

\paragraph{Data Description}
The Craters Wildlife Observation (CWO) database~\cite{craters-of-the-moon-wildlife} contains observations of wildlife spotted at the Craters of the Moon national monument and preserve.
It is the smallest and most natural database in the benchmark data set.

% \begin{figure}[H]
%   \centering
%   \includegraphics[width=\linewidth]{figures/query-stats/CratersWildlifeObservations-stats-clause-counts.png}
%   \caption{Gold query clause composition - CWO database}
%   \label{fig:query-complexity-cwo}
% \end{figure}

\paragraph{CWO Database Technical Details}
\begin{itemize}
  \item Data source format: Microsoft Access
  \item Migration method: SQL Server Migration Assistant
  \item Table count: 13
  \item Column count: 71
  \item Mean columns per table: 5.461
  \item NL Questions: 40
  \item Combined naturalness level: 0.84 
\end{itemize}

\subsubsection{Northern Great Plains Fire Management: FFI Database}

\paragraph{Data Description}
The NPFM database~\cite{ngp-fire} contains observations of various overstory and other flora within the Northern Plains region of the Nationa Parks Service.

% \begin{figure}[H]
%   \centering
%   \includegraphics[width=\linewidth]{figures/query-stats/NorthernPlainsFireManagement-stats-clause-counts.png}
%   \caption{Gold query clause composition - NPFM database}
%   \label{fig:query-complexity-npfm}
% \end{figure}

\paragraph{NPFM Database Technical Details}
\begin{itemize}
  \item Data source format: Microsoft Access
  \item Migration method: SQL Server Migration Assistant
  \item Table count: 27
  \item Column count: 190
  \item Mean columns per table: 7.037
  \item NL Questions: 40
  \item Combined naturalness level: 0.70 
\end{itemize}

\subsubsection{2021 Crash Investigation Sampling System}

\paragraph{Data Description}
The crash investigation sampling system~\cite{crash-investigation-sampling-system} is sourced from the National Transportation Safety Board, and referred to as NTSB in this paper.
It contains sampled data of crash and vehicle statistics from 2021.
The data is organized such that composite key joins are required for most multi-relation queries.

\paragraph{Additional Implementation Details}
This is the only database in our collection that required deliberate migration from a non-database format to the target MS SQL Server environment.
We acquired the data in .csv form, with a single .csv per table.
Analysis of the files confirmed that although not in database form, the data was relational in nature, and migration involved SQL-based ingestion from .csv files into the target schema.
The .sql scripts used to generate the database schema and insert table values are available in the project repository.

% \begin{figure}[H]
%   \centering
%   \includegraphics[width=\linewidth]{figures/query-stats/NTSB-stats-clause-counts.png}
%   \caption{Gold query clause composition - NTSB database}
%   \label{fig:query-complexity-ntsb}
% \end{figure}

\paragraph{NTSB Database Technical Details}
\begin{itemize}
  \item Data source format: Comma Separated Value (CSV) files
  \item Migration method: SQL database creation and Python-based ETL scripting
  \item Table count: 40
  \item Column count: 1,611
  \item Mean columns per table: 40.275
  \item NL Questions: 100
  \item Combined naturalness level: 0.59 
\end{itemize}


\subsubsection{New York State Education Department Report Card Database 2021-22}

\paragraph{Data Description}
The NYSED database~\cite{nysed-report-card} is sourced from the New York State Education Department.
It contains standardized testing and demographic data for all public elementary, middle, and high schools in New York State.

% \begin{figure}[H]
%   \centering
%   \includegraphics[width=\linewidth]{figures/query-stats/NYSED_SRC2022-stats-clause-counts.png}
%   \caption{Gold query clause composition - NYSED database}
%   \label{fig:query-complexity-nysed}
% \end{figure}

\paragraph{NYSED Database Technical Details}
\begin{itemize}
  \item Data source format: Microsoft Access
  \item Migration method: SQL Server Migration Assistant
  \item Table count: 27
  \item Column count: 423
  \item Mean columns per table: 15.67
  \item NL Questions: 63
  \item Combined naturalness level: 0.68 
\end{itemize}

\subsubsection{Localized Demo Databases Now Available for SAP Business One}

\paragraph{Data Description}
The SBOD database~\cite{sap-demo} is sourced from a publically available SAP demonstration and training database.
It is the largest, and least natural, database within our dataset.
Given its schema size, we divided it based on SAP module, and further reduced the schemas used in our benchmark based on the training database cardinality (e.g. we removed most tables containing 0 tuples).


% \begin{figure}[H]
%   \centering
%   \includegraphics[width=\linewidth]{figures/query-stats/SBODemoUS-stats-clause-counts.png}
%   \caption{Gold query clause composition - SBOD database module example}
%   \label{fig:query-complexity-sbod}
% \end{figure}

\paragraph{SBOD Database Technical Details}
\begin{itemize}
  \item Data source format: MS SQL Server backup (.bak) file
  \item Migration method: MS SQL Server backup recovery
  \item Table count: 2,588
  \item Column count: 90,477
  \item Mean columns per table: 34.96
  \item NL Questions: 100
  \item Combined naturalness level: 0.49 
\end{itemize}

\begin{table}[th]
  \centering
  \caption{SBO Demo Module Schemas}
  \begin{tabular}{p{4cm}ccc}
  % \hline
  \toprule
  \textbf{Module} & \textbf{Tables} & \textbf{Columns} & \textbf{Questions} \\
  % \hline
  \midrule
  Banking & 40 & 1720 & 10 \\
  Business Partners & 40 & 1443 & 10 \\
  Finance & 61 & 1988 & 10 \\
  General & 71 & 1035 & 10 \\
  Human Resources & 28 & 440 & 20 \\
  Inventory and Prod. & 65 & 1942 & 10 \\
  Reports & 40 & 734 & 10 \\
  Sales Opportunities & 20 & 283 & 10 \\
  Service & 40 & 875 & 10 \\
  % \hline
  \bottomrule
  \end{tabular}
  \label{table:sapmodulechemas-appendix}
\end{table}

\paragraph{SAP Business One Additional Details}
Business One is an enterprise resource planning (ERP) system created by the German software systems developer SAP.
It is a common platform in government and commercial domains where large-scale business management solutions are required.
The SBOD database contains a significant number of tables and columns.
Such a schema size poses a problem for generating schema knowledge representations in zero shot prompts, even for large context window variations of evaluated LLMs.
To overcome this constraint, we divide the SBOD schema into 9 sub-modules based on schema descriptions published by an online community of SAP practicioners~\cite{sap-erpref}.
We further prune the SBOD schemas using the cardinality of the training database, where tables without data entries were excluded from NL questions and prompt schema knowledge.



\subsection{Benchmark Naturalness Comparisons}

Though we believe the quality of existing benchmarks is excellent, and the hard work of researchers associated with those projects has resulted in significant improvements in NL-to-SQL system design, we find that a large proportion of these benchmark schemas are canonical, small in composition, and highly natural compared to databases and data sets often encountered in real world scenarios.
Using our naturalness classifier, we determine the naturalness levels of the Spider~\cite{Yu&al.18c} and Bird-SQL~\cite{li2023llm}.
Our classifier model indicates that both Spider and Bird database schemas are highly natural, moreso than any real-world schema we acquired for our benchmark (see Figure \ref{fig:benchmark-naturalness-comparison-appendix} for a visual comparison).
Additionally, we evaluate the naturalness of the real-world schema identifiers in the SchemaPile~\cite{doehmen2024schemapile} and find that the SNAILS naturalness better-aligns with SchemaPile than the previously-mentioned benchmarks.
Figure \ref{fig:databasenaturalness}

\begin{figure}
  \centering
  \includegraphics[width=0.7\linewidth]{figures/benchmark_naturalness_compare.pdf}
  \caption{Spider~\cite{Yu&al.18c} and Bird~\cite{li2023llm} benchmarks classified with the Davinci-based classifier reveals that both benchmark databases have highly natural identifiers even compared to the most natural of the databases in our proposed benchmark. Our benchmark more closely aligns with the naturalness of the real-world schema collection SchemaPile~\cite{doehmen2024schemapile}}.
  \label{fig:benchmark-naturalness-comparison-appendix}
\end{figure}


\subsection{NL Question - SQL Query Pairs}

To evaluate SQL inference performance over the Native and modified schemas in the SNAILS real-world database collection, we create a new set of 503 NL-question and SQL gold query pairs (Artifact 6).
Schema identifier naturalness are the primary considerations for NL question and gold query composition.
During question and query formulation we track schema coverage to ensure that the distribution of identifier naturalness within a set of gold queries generally matches the naturalness distribution of target schemas.

To enable accuracy measurements at the identifier level, gold queries contain the minimum identifiers (tables and columns) required to answer its corresponding question.
For this reason, for questions that require the count aggregation function, where appropriate, we use the COUNT(*) clause (as opposed to selecting an arbitrary column).
This approach eliminates incorrect penalties to recall if a generated query fails to project an arbitrary column as a function argument.

Gold queries contain only native identifiers, such that all gold queries return valid non-null results from target databases in the real-world database collection (Artifact 1).
We measure query complexity as a count of its clauses and identifiers.
Gold queries span a range of complexities, from very simple single table projections, to multi-table joins and nested subqueries (see Table \ref{table:query-stats}).

\begin{table*}[t]
  \centering
  \caption{Gold query clause counts for each SNAILS database. Columns represent a count of gold queries that contain the listed clause types. Qs is the count of question-query pairs for each database. C-Join is the subset of joins that require a composite key. Ex indicates the use of an exists clause. SQ indicates a subquery. Neg, Grp, Ord, and Hvg represent negation, group by, order by, and having. Note: MS SQL Server dialect replaces the common LIMIT clause with an equivalent TOP clause that precedes select items in the SELECT clause.}
  \begin{tabular}{lrrrrrrrrrrrr}
\toprule
Database & Qs & Top & Funct. & Join & C-Join & Ex & SQ & Where & Neg & Grp & Ord & Hvg \\
\midrule
ASIS & 40 & 1 & 24 & 13 & 1 & 0 & 2 & 18 & 0 & 17 & 1 & 0 \\
ATBI & 40 & 5 & 20 & 18 & 0 & 1 & 7 & 21 & 2 & 16 & 7 & 1 \\
CWO & 40 & 2 & 18 & 5 & 1 & 5 & 10 & 34 & 7 & 12 & 2 & 1 \\
KIS & 40 & 8 & 26 & 15 & 0 & 0 & 2 & 25 & 1 & 11 & 8 & 0 \\
NPFM & 40 & 5 & 27 & 21 & 0 & 0 & 1 & 29 & 0 & 16 & 5 & 0 \\
NTSB & 100 & 8 & 82 & 23 & 21 & 0 & 6 & 62 & 4 & 42 & 23 & 4 \\
NYSED & 63 & 10 & 36 & 10 & 4 & 1 & 21 & 55 & 1 & 16 & 10 & 1 \\
PILB & 40 & 6 & 25 & 23 & 0 & 0 & 3 & 20 & 0 & 16 & 11 & 2 \\
SBOD & 100 & 2 & 33 & 44 & 0 & 0 & 0 & 82 & 0 & 17 & 2 & 1 \\
\bottomrule
\end{tabular}

  \label{table:query-stats}
\end{table*}

The NL question - SQL query pair artifact consists of 9 .sql files containing between 40 and 100 entries each.
Question and query pairs are written in executable .sql files.
Natural language questions are written as SQL comments; and SQL is written in the T-SQL dialect employed in MS SQL Server.
For public repositories storing the questions, we store them in .zip files in order to reduce the possibility of inclusion in language model training material.
Each file is associated with a database in the SNAILS schema collection.
Some NL questions contain hint and note entries annotated as HINT and NOTE in lines that follow the NL query.
We used neither the hints nor columns in any of the experiments described in this paper, but retain them for possible use in future research.

While we store the data in .sql file format for readability and ease of use, we also offer an NL question loading script (load\_nl\_questions.py) in our repository. 
This script performs rudimentary parsing of the .sql files and returns a Pandas DataFrame and optional .xlsx formatted spreadsheet.

\paragraph{NL Question - SQL Query Example 1, ASIS Database Question 8}
The focus of this benchmark dataset is on the evaluation of schema linking.
As such, we were generous with value descriptions, providing literal value strings (e.g. ASIS\_HERPS\_20H location code in example 1) in the prompt.

\begin{figure}[H]
\begin{Verbatim}[frame=single,fillcolor=\color{lightgray}]
-- 8: show how many minnows of each stage were counted 
      at the location ASIS_HERPS_20H
SELECT stage, sum(count) minnowCountSum
FROM tblFieldDataMinnowTrapSurveys
WHERE locationID = 'ASIS_HERPS_20H'
GROUP BY stage
;
\end{Verbatim}
\caption{NL Question - SQL Query Example 1, ASIS Database Question 8}
\end{figure}

\paragraph{NL Question - SQL Query Example 2, NTSB Database Question 13}
Example 2 shows additional code value hints provided in the NL question.
In order to enable the recall evaluation statistic, we limited the use of columns and tables in gold queries to the minimum necessary to form a correct query.
In the cases where any arbitrary column as an argument in the count function will yield the same result as the $\ast$, we use only the $\ast$ symbol. 
This eliminates the recall penalty for models selecting an arbitrary column within the count function.


\begin{figure}[H]
\begin{Verbatim}[frame=single,fillcolor=\color{lightgray}]
-- 13: How many vehicles are there where drugs were present 
       (presence code value is 1) and the vehicle was towed 
       for a reason not due to disabling damage (towed code is 3)
SELECT COUNT(*) VEHCOUNT
FROM GV
WHERE PARDRUG = 1 AND TOWED = 3
;
\end{Verbatim}
\caption{NL Question - SQL Query Example 2, NTSB Database Question 13}
\end{figure}


\paragraph{NL Question - SQL Query Example 3, SBOD Database Human Resources Module Question 18}
Questions vary in their complexity.
This example shows one of the more complex questions that require multiple projections and joins as well as a selection.

\begin{figure}[H]
\begin{Verbatim}[frame=single,fillcolor=\color{lightgray}]
-- 18: Show the professional status and educational 
       statuses as well as the home and work street 
       numbers of employees on the purchasing team.
SELECT StatusOfP, StatusOfE, StreetNoW, StreetNoH
FROM OHEM employees
JOIN HTM1 teamMembers 
    ON employees.empId = teamMembers.empID
JOIN OHTM emplTeams 
    ON teamMembers.teamID = emplTeams.teamID
WHERE emplTeams.name = 'Purchasing'
;
\end{Verbatim}
\caption{NL Question - SQL Query Example 3, SBOD Database Human Resources Module Question 18}
\end{figure}


\paragraph{\textbf{Adding New NL-SQL Pairs to the SNAILS Collection}}
For researchers interested in extending the SNAILS collection, it is necessary to create new ground truth NL-SQL pairs for evaluation. 
While we employed a fully manual approach for question writing, and this approach may be used for future additions, they may also consider the use of new approaches such as using a template-based approach for generating question-query pairs with relational data as input~\cite{10.5555/3666122.3667470}.
Regardless of NL-SQL pair creation method, researchers should ensure adequate schema coverage and minimum essential identifier selection as described in the preceding section.


\section{NL-to-SQL Benchmarking Setup}

\label{section:nl-to-sql-benchmarking-setup}

\begin{figure}[!h]
  \centering
  \includegraphics[width=\figwidthmod\linewidth]{figures/section-5-process-header.pdf}
  \caption{Experiment setup workflow from NL question and schema as input to predicted query as output.}
  \label{fig:section-5-process-header}
\end{figure}

To evaluate the impact of naturalness on NL-to-SQL accuracy, we build a benchmarking setup pipeline as shown in Figure \ref{fig:section-5-process-header}. 
NL question and gold query pairs, database schemas, and naturalness crosswalk mappings are inputs into subprocesses.
The subprocesses include prompt generation, schema identifier naturalness modification, identifier naturalness classification, LLM-based NL-to-SQL inference, and predicted query ``denaturalization'' (i.e., converting table and column identifiers to native schema identifiers prior to query execution).
The output of the experiment setup is a predicted query, which along with its gold query counterpart, is executed against a target database. 
This predicted query is passed into a parser analysis tool as initial steps of the \emph{Performance Evaluation and Results Classification} phase of the experiment described in Section \ref{section:nl-to-sql-benchmarking-results}.

\subsection{Prompt Generation}
The design space for LLM-based NL-to-SQL prompting is quite large, with options ranging from zero-shot instructions to sequential prompting broken into discrete tasks such as schema subsetting and error handling.
Although we evaluate 2 complex NL-to-SQL workflows, to maintain consistency across the LLMs compared in this study, our performance comparisons focus on a single prompting strategy: zero-shot prompting with schema knowledge.

\paragraph{\textbf{Prompting Strategy}}

SNAILS prompts consist of zero-shot  instructions with schema knowledge (denoted as ZS in results figures) in a format similar to OpenAI's Text-to-SQL demonstration prompt~\cite{gao2023texttosql} for completions. 
The prompt begins with task instructions and database information:

\begin{verbatim}
  For the database described next, provide only a sql query. 
  do not include any text that is not valid SQL.
  #Database: NTSB
  #MS SQL Server tables, with their properties:
\end{verbatim}

Target database system tables provide schema knowledge, which is represented as a list of tables and their column names with data types in the format:

\begin{verbatim}
  #TableName (Col1Name Type, Col2Name Type, ...)
\end{verbatim}

The prompt ends with the instruction:

\begin{verbatim}
  ### a sql query, written in the MS SQL Server dialect, 
      to answer the question: <Question>
\end{verbatim}

Where <Question> is replaced with an NL question directed at the given schema.

To evaluate naturalness effects on more complex NL-to-SQL prompting workflows, we also implement DIN SQL~\cite{pourreza2023dinsql} which uses prompt chaining with GPT-4, and CodeS~\cite{10.1145/3654930}--a multi-step NL-to-SQL system (schema filtering and SQL inference) based on StarCoder~\cite{li2023starcoder} and finetuned for the NL-to-SQL translation task.

\paragraph{\textbf{Prompt Schema Identifier Modification}}
For inference on virtual schemas with modified naturalness levels, we replace Native identifiers with corresponding identifiers of the target virtual schema's naturalness level.
We accomplish this step using the naturalness-modified identifier collection (Artifact 4) described in Section \ref{subsection:naturalnessmapping}.
We use a SQL parser to encase identifiers within tags to improve identifier replacement accuracy and eliminate errors due to substring matching between identifiers.


\subsection{Prompt Naturalness Modification}

Prompt naturalness modification is necessary when generating SQL queries over schemas with modified identifiers.
In order to prevent producing additional database instances with renamed identifiers, we employ a middleware approach where modified identifiers are retrieved from a mapping of native identifiers to the target naturalness level.
The prompt is generated using native schema identifiers, and table and column names are encased in XML-like opening and closing <TABLE\_NAME> and <COLUMN\_NAME> tags.
Native identifiers and their tags are replaced with modified identifiers using standard Python string replacement (e.g. str.replace(target, value)).

\paragraph{Prompt naturalness conversion example}
In this example, the objective is to modify the naturalness of the Klamath Invasive Species (KIS) schema to the least natural form.
The first step is to generate a tagged prompt which is formed using the database metadata accessed via system tables.
A tagged prompt table with columns and datatypes takes on the form:
\begin{verbatim}
#<TABLE_NAME>tlu_Species_WHIS</TABLE_NAME>
( 
  <COLUMN_NAME>Species</COLUMN_NAME> nvarchar, 
  <COLUMN_NAME>SampleYear</COLUMN_NAME> nvarchar, 
  <COLUMN_NAME>Park</COLUMN_NAME> nvarchar
)
\end{verbatim}
Each table and its columns occupies a single line within the prompt.
The resulting prompt after string replacement appears as the following in the final prompt:
\begin{verbatim}
#TSW( Sp nvarchar, S_Yr nvarchar, Pk nvarchar)
\end{verbatim}

During NL-to-SQL translation, the modified schema knowledge representation is presented to the LLM as if it is the native schema.


\subsection{NL-to-SQL Inference}

\paragraph{\textbf{Language Models}}
Foundational LLMs continue to grow in capability at a rapid pace.
Despite this growth, not all NLI implementations can avail of the most-capable LLMs, often due to organizational policy constraints (e.g., organizational security concerns~\cite{gsa-llm-directive}).
Additionally, we seek to understand if schema naming effects generalize across model architectures and sizes.
Thus, we consider several LLMs, both open and closed source, to capture as many use profiles as possible including OpenAI's GPT-3.5 Turbo and GPT-4o~\cite{openai-chatgpt-blog-post, openai-api-documentation}; Google's Gemini 1.5 Ultra~\cite{geminiteam2024gemini, geminiteam2024gemini15}; and Phind-CodeLlama-34B-v2~\cite{phind2022phindcodellama} which is a finetuned variant of Meta's CodeLlama 2~\cite{roziere2023code}.

\paragraph{GPT 3.5 Turbo and GPT 4o}
GPT-based~\cite{openai-chatgpt-blog-post} generations use the gpt-3.5-turbo-16k model accessed using OpenAI's API services~\cite{openai-api-documentation}. 
GPT-based models employ BPE using tiktoken~\cite{openai-tokenizer} to tokenize inputs and decode model outputs.

Due to the size of larger schema knowledge prompts, we make use of the GPT 3.5 model~\cite{openai-chatgpt-blog-post} with a context window of 16,000 tokens \emph{gpt-3.5-turbo-16k}.
For consistency, all queries were generated with a 0 temperature, 1 top p, 0 frequency penalty, 0 presence pentalty.
Responses are fetched from the OpenAI Python \emph{ChatCompletion} class' \emph{create} method with a single message (prompt) passed in the \emph{user} role. 

\paragraph{Phind Code-Llama}
Phind Code-Llama~\cite{phind2022phindcodellama} is a fine-tuned version of the 34b parameter Code Llama model.
We used the TogetherAI API to access this model.
We find that the finetuning appears to have improved its performance as compared to the baseline Code Llama 34b version, and as such we replace our prior analysis using Code Llama 34b with the results generated using the Phind model.

\paragraph{Gemini 1.5 Pro}
During the course of our research, Google released Gemini 1.5 Pro~\cite{geminiteam2024gemini15}, and we quickly integrated it into our existing workflows using the Google generative AI Python library.
A remarkable development of the Gemini 1.5 model is its context window size of 1 million tokens, which negates the need to reduce schema knowledge representations for the database schemas in the SNAILS collection in order to meet context window constraints.

\paragraph{\textbf{CodeS and DIN SQL Implementation}}
For the more complex DIN SQL and CodeS NL-to-SQL workflows, we provide additional versions of the SNAILS schema artifacts to conform to the input requirements of the target systems.
Additionally, we add data logging between agents to document the schema filtering step for additional analysis.
For consistency between approaches, we use GPT-4o for all steps in the prompting chain.
For CodeS inference, we execute the schema filtering and NL-to-SQL inference using the CodeS codebase and finetuned models.

\paragraph{\textbf{Generated Query Denaturalization}}
For queries targeted at virtual schemas and generated using modified schema identifiers, we perform reverse modifications prior to query execution on the native database schema.
Using a purpose-built Antlr~\cite{Parr2014}-based parser, we extract table and column identifiers, and generate a tagged query with identifier tags encasing table and column names.
The tags guide the replacement algorithm, ensuring accurate replacement of naturalness-modified identifiers with their Native naturalness counterparts.


\subsection{Query Naturalness Modification}
\label{section:querynaturalnessmodification}

When a query is formed against a schema with naturalness-modified identifiers, it is necessary to replace the modified identifiers with the native identifiers prior to executing the query over the target database.
Simple string replacement is not sufficient in this case, because some identifier names may be substrings of other identifiers within a query.
We employ a Java-based parser and AST generator~\cite{Parr2014,antlrgrammarsv4} to build a parser system for tagging table and column identifiers in a query.

\paragraph{Query naturalness modification example}

To answer the question \emph{For each location type, show a count of locations in shasta county}, GPT 3.5 generated the query with lowest naturalness schema knowledge:

\begin{verbatim}
SELECT LcTp, COUNT(*) AS LocationCount
FROM Locs
WHERE Cty = 'Shasta County'
GROUP BY LcTp 
\end{verbatim}

This query is converted to all capital characters and passed to the query parser tagger via API.
The tagger traverses the AST using a listener class to encase tables and columns with identifier opening and closing tags. The parser also returns a list of aliases generated within the query.
This allows consuming systems to ignore tagged aliases within the tagged query:

\begin{verbatim}
@BEGINTAGGEDQUERY
SELECT 
  <COLUMN_NAME> LCTP </COLUMN_NAME> , 
  COUNT ( * ) AS LOCATIONCOUNT 
FROM <TABLE_NAME> LOCS </TABLE_NAME> 
WHERE 
  <COLUMN_NAME> CTY </COLUMN_NAME> 
    = 'SHASTA COUNTY' 
GROUP BY 
  <COLUMN_NAME> LCTP </COLUMN_NAME>
@ENDTAGGEDQUERY
@BEGINALIASES
<COLUMN_ALIASES>LOCATIONCOUNT</COLUMN_ALIASES>
@ENDALIASES  
\end{verbatim}

With the tagged query and aliases, the query is converted into a form that is compatible with the target native schema. This is accomplished using the schema identifier mapping dataset and standard Python string operations (e.g. str.replace("<TABLE\_NAME> LOCS </TABLE\_NAME>", "TBL\_LOCATIONS")).

\begin{verbatim}
@DENATURALIZED RESPONSE:
SELECT 
  [LOC_TYPE], 
  COUNT (*) AS LOCATIONCOUNT 
FROM [TBL_LOCATIONS] 
WHERE [COUNTY] = 'SHASTA COUNTY' 
GROUP BY [LOC_TYPE]   
\end{verbatim}

This completes the query modification step.
The modified query is then used to extract results from the target database for result set matching evaluation.




\section{Performance Evaluation}


\subsection{Query Execution}

Predicted and gold queries are executed over target databases with native schemas using the PyOdbc Python library.
Valid result sets are stored as Pandas Dataframes for comparison.

\subsection{Execution Result Set-Subset Matching}
\label{section:resultsetmatching}

Result set and subset comparison consisted of a comparison of the columns and rows returned by the two queries. Because it is possible for semantically equivalent query results to differ in terms of column ordering, column and aggregate function aliasing, row ordering (when an order is not specified in the question), and even the number of columns returned (as long as the predicted column set is a superset of the gold column set), result set comparison was performed using a series of rules implemented in Python.

\textbf{Result cardinality} The number of tuples, denoted as $|C_G|$ and $|C_C|$ in the predicted result $R_P$ and the gold result $R_G$ must be equal, and must be greater than 0:
$$\forall C_G \in R_G, C_P \in R_P \left(|C_G| = |C_P|\right) \land (|C_G| > 0 \land |C_P| > 0)$$
Empty sets are tagged as undetermined and retained for syntactic comparison. Non-empty, non-equal-size sets are tagged as non-matches and witheld from further analysis.

\textbf{Projection completeness} The columns in the predicted result $R_P$ must be a superset of the columns in the gold result $R_G$. Columns $C_P$ and $C_G$ equivalence is determined by value comparisons between tuples $T_G$ and $T_P$ of the columns' contents:
\[\forall C_G \in R_G, \exists C_P \in R_P \text{ such that } \forall T_G \in C_G, T_G \in C_P\]
Column match candidates are determined via pairwise comparison of the sorted values in each column in $R_G$ to each column in $R_P$. Full result sets $R_G$ and $R_P$ are sorted by corresponding column match candidates, with columns containing the most unique values serving as the primary sort key. With both $R_G$ and $R_P$ sorted by column match candidates, the two sets are compared row-wise for all columns in the set of column match candidates. If the two sets are not equal, the result sets are considered semantically non-equivalent.


\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{figures/manual-validation-tool.PNG}
  \caption{Screenshot of the query manual validation tool. This example depicts a query that passed set-subset matching, and is currently classified as ungraded. Helper information indicates that although the results matched, the incorrect table was selected during inference (AHEM instead of OHEM). This example was classified as incorrect.}
  \label{fig:manual-validation-tool}
\end{figure}

\subsection{Human Evaluation}
Predicted queries that pass execution result set-subset matching are further evaluated by a human researcher to rule out false positives.
Predictions that fail result set comparison are pre-classified as also failing manual matching.
Predictions that pass set comparison are classified as ungraded until reviewed by a human researcher.
Once reviewed using the Python-based GUI evaluation tool (see Figure \ref{fig:manual-validation-tool}), the predicted query receives its final matching score.
Human evaluation resulted in scoring as incorrect 41 predictions that passed result set matching which is approximately two percent of all result set matched queries.




\subsection{Schema Linking End-to-end Example}

To better pinpoint schema linking performance, we devise a new approach for evaluating NL-to-SQL generation.
In this approach, performance is measured using set comparisons between sets of identifiers within gold and predicted SQL queries.
Recall is the primary metric.
Precision and F1 are available, but less helpful, due to penalization for additional predicted columns that do not cause a query to be incorrect.
The formulae for deriving linking metrics are printed in subsection \ref{section:linking-evaluation} of the main report.

\paragraph{\textbf{Linking Evaluation Example (ATBI Question 30, CodeLlama-34b)}}
This is an example of a predicted query that fails result set match comparison.

\paragraph{Natural Language Question}
Which tree species were recorded as mature overstory but not as saplings? Include the species name and common name.

\paragraph{Gold Query}
The gold query is quite complex, where using EXISTS clauses is a possible approach for solving.
\begin{verbatim}
  SELECT species, CommonName 
  FROM tlu_PlantSpecies sp 
    WHERE EXISTS(
      SELECT overstory_id  	
      FROM tbl_Overstory  	
      WHERE SpCode = sp.SpeciesCode
    ) AND NOT EXISTS (
      SELECT Seedlings_ID  	
      FROM tbl_Seedlings  	
      WHERE SpCode = sp.SpeciesCod
    )
\end{verbatim}

\paragraph{Predicted Query (Code Llama 34b)} 
As we can see below, Code Llama 34b did not provide a correct solution.
However, failure of result set matching does not preclude us from gleaning useful information from the result.
\begin{verbatim}
SELECT DISTINCT
	tlu_PlantSpecies.genus,
	tlu_PlantSpecies.subgenus,
	tlu_PlantSpecies.species,
	tlu_PlantSpecies.subspecies,
	tlu_PlantSpecies.SpeciesCode,
	tlu_PlantSpecies.CommonName
FROM
	tlu_PlantSpecies
	LEFT JOIN tbl_Overstory 
    ON tbl_Overstory.SpCode 
      = tlu_PlantSpecies.SpeciesCode
	LEFT JOIN tbl_Saplings 
    ON tbl_Saplings.SpCode 
      = tlu_PlantSpecies.SpeciesCode
WHERE
	tbl_Overstory.SpCode IS NOT NULL
	AND tbl_Saplings.SpCode IS NULL
ORDER BY
	tlu_PlantSpecies.genus,
	tlu_PlantSpecies.subgenus,
	tlu_PlantSpecies.species,
	tlu_PlantSpecies.subspecies,
	tlu_PlantSpecies.SpeciesCode,
	tlu_PlantSpecies.CommonName
\end{verbatim}

\paragraph{Identifier Set Extraction}
Using the purpose-built T-SQL parser, we extract a set of identifiers from the gold and predicted queries.
Note that we do set comparison; so although columns are referenced in multiple clauses in the predicted query, we only measure the presence of a column or table once.

With a set $QI_g$ of identifiers present in the gold query and a set of identifiers $QI_p$ present in the generated (or predicted) query, we calculate recall, as well as f1 and precision.

Gold query identifiers $QI_g :=$ 
  \begin{verbatim}
  {
  'TLU_PLANTSPECIES', 'TBL_OVERSTORY', 'TBL_SEEDLINGS',
  'SPECIES', 'SPECIESCODE', 'COMMONNAME', 'SPCODE',
  'OVERSTORY_ID', 'SEEDLINGS_ID'
  }
\end{verbatim}

Predicted query identifiers $QI_p :=$ 
\begin{verbatim}
  {
  'TLU_PLANTSPECIES', 'TBL_OVERSTORY', 'TBL_SAPLINGS'
  'SPECIES', 'SPECIESCODE', 'COMMONNAME', 'SPCODE',
  'GENUS', 'SUBSPECIES', 'SUBGENUS'
  }
\end{verbatim}

\paragraph{Identifier Set Comparisons}

True positives are the intersection $QI_g \cap QI_p =$
\begin{verbatim}
  {
  'TLU_PLANTSPECIES', 'TBL_OVERSTORY',
  'SPECIES', 'SPECIESCODE', 'COMMONNAME', 'SPCODE'
  }
\end{verbatim}

$$QueryRecall = \frac{|QI_g \cap QI_p|}{|QI_g|} = \frac{6}{9} = 0.667$$
$$QueryPrecision = \frac{|QI_g \cap QI_p|}{|QI_p|} = \frac{6}{10} = 0.60$$
$$QueryF1 = \frac{2(Recall*Precision)}{Recall+Precision} = 0.632$$

So we see that although the predicted query failed in terms of execution result set comparison, we can still grade it in terms of linking performance.
In other words, we can assign \emph{partial credit} to predicted queries where correct schema identifiers are recalled.





\section{NL-to-SQL Benchmarking Results}

\label{section:nl-to-sql-benchmarking-results}

This section describes the process of evaluating the generated SQL query output from the prior section.
We evaluate performance in terms of execution accuracy (result set comparison and manual evaluation) and schema linking (recall, precision, and F1).

\begin{figure}[!h]
  \centering
  \includegraphics[width=\figwidthmod\linewidth]{figures/section-6-process-header.pdf}
  \caption{Benchmark results evaluation includes generated and gold query execution on target schemas, parser-based analysis, and identifier set comparisons. We evaluate performance in terms of execution accuracy and schema linking (precision, recall, and F1).}
\end{figure}

\paragraph{\textbf{Key Takeaways}}
Overall, there is a model-dependent statistically significant correlation between identifier naturalness and execution accuracy, with smaller models exhibiting higher correlations between naturalness and performance.
The presence of Least naturalness identifiers has the largest negative effect on schema linking.
Additionally, while the performance difference between Regular and Low is visible, it is less impactful. 
So, modifying Least naturalness identifiers should be a  higher priority than modifying Low naturalness identifiers.

\subsection{Execution Accuracy}

\begin{figure}
  \centering
  \includegraphics[width=\figwidthmod\linewidth]{figures/execution-accuracy-barplot.pdf}
  \caption{Execution accuracy (proportion of correct queries) by model. There is slight accuracy improvement from native schemas to schemas modified to regular naturalness. Accuracy drops significantly for schemas modified to low naturalness.}
  \label{fig:execution-accuracy-barplot}
\end{figure}

\paragraph{\textbf{Execution Result Set Comparison}}
Execution accuracy is the standard measure of performance in most NL-to-SQL benchmarks~\cite{li2023llm, Yu&al.18c} where accuracy is determined using result set comparisons between gold and generated queries executed over one or more database instances.
A drawback of existing methods is that strict set or bag comparisons risk increased false-negatives when a generated query includes additional fields that are not required, but do not render the result incorrect in terms of the natural language question~\cite{10023434, floratou2024nl2sql}.

To reduce false negatives, the SNAILS approach to execution accuracy evaluation adopts 2 aspects of relaxed execution matching as described in~\cite{floratou2024nl2sql}; it accounts for: (1) The possibility that a predicted query may contain additional columns beyond those retrieved by a gold query; and (2) That unless specified in the NL question, tuples may appear in any order.
To achieve this, we perform result set-superset comparisons to ensure that the predicted result set column set is a superset of the gold result set column set.
That is, a generated query is considered incorrect if it does not contain \emph{all} gold query columns; but it is not considered incorrect (at this stage) if it includes columns not present in the gold query result. 
A more detailed description of this approach is described previously in Section~\ref{section:resultsetmatching}.

\paragraph{\textbf{Manual Evaluation}}
Execution result set comparison cannot prove query correctness; so we rely on it only to rule out true negatives from further consideration.
To validate correctness, the authors manually review generated queries that pass execution result set-superset comparison checks.
We streamline this process by creating a Python-based manual validation user interface that makes the process of comparing gold and generated queries more user-friendly.
Manual validation steps include ensuring the generated query answers the NL question, matches the gold query in terms of semantic structure, and does not contain semantically incorrect predicates, projections, or clauses.

\paragraph{\textbf{Naturalness Effect on Execution Accuracy}}

Figure \ref{fig:execution-accuracy-barplot} shows execution accuracy for each LLM and naturalness level.
There is a clear difference in overall performance between LLMs, most likely due to model size.
We find that generally more natural database schemas yield more correct queries.
Databases with more natural native schemas did not benefit from identifier renaming, though we observe that altering a schema to become less natural degrades accuracy in most cases. We find that for databases with Native schema combined naturalness scores less than 0.69, modifying the schema identifiers to increase naturalness improves execution accuracy.

\paragraph{\textbf{Statistical Significance}}
The Kendall-Tau correlation between the naturalness of identifiers in a query and execution accuracy ranges from low ($\tau = 0.09, p < 0.0001$) for Gemini 1.5, to moderate ($\tau = 0.19, p < 0.0001$) for Phind-CodeLlama2 and CodeS.
The most impactful relationship is between the presence of Least naturalness identifiers and performance, with Kendall-Tau ($\tau$) Correlations between the proportion of Least  identifiers in a query and execution accuracy between $\tau=-.15$ and $\tau=-.22$ with $p < 0.0001$ for all models.

\subsection{Schema Linking Evaluation}
\label{section:linking-evaluation}

We make schema linking a ``first class citizen'' of our analysis, and study schema linking performance in queries irrespective of other aspects of correctness. 
Thus, we propose query-level and identifier-level schema linking measurements.
We propose an approach similar to the Spider benchmark exact set matching system~\cite{Yu&al.18c} in which we employ a schema linking-specific evaluation method using \emph{recall} scoring of gold and generated query pairs.
Other schema linking-focused research measure effects of schema linking improvements using ablation~\cite{wang2020rat-sql, cao-etal-2021-lgesql, 10.1145/3534678.3539305, 49288}. 
In other cases, schema linking is described in post-hoc analysis of NL-to-SQL model performance, with schema linking accounting for roughly 30\% of failures~\cite{dong2023c3, pourreza2023dinsql}.

\paragraph{\textbf{Query-Level Linking Analysis}}
The set of all schema identifiers (table and column names) present in gold queries represents the minimum identifiers required to correctly answer an NL question.
Our purpose-built ANTLR4-based~\cite{Parr2014} query parser extracts identifiers from gold and generated queries.
With a set $QI_g$ of identifiers present in the gold query and a set of identifiers $QI_p$ present in the generated (or predicted) query, we calculate recall, as well as F1 and precision.

\begin{equation}
  \label{eq:recall}
  % Recall = \frac{|I_g \cap I_p|}{|I_g \cap I_p| + |I_g - I_p|}
  QueryRecall = \frac{|QI_g \cap QI_p|}{|QI_g|}
\end{equation}

\begin{equation}
  \label{eq:precision}
  % Precision = \frac{|I_g \cap I_p|}{|I_g \cap I_p| + |I_p - I_g|}
  QueryPrecision = \frac{|QI_g \cap QI_p|}{|QI_p|}
\end{equation}

\begin{equation}
  \label{eq:f1}
  QueryF1 = \frac{2(QueryRecall*QueryPrecision)}{QueryRecall+QueryPrecision}
\end{equation}
\\
We exclude 137 linking score calculations from analysis in situations where the predicted query contains invalid SQL that prevents query parsing and identifier extraction.
We use recall as the primary measure for schema linking, as it does not penalize generated queries that contain extra identifiers that do not render an answer incorrect in our setting, such as cases when an arbitrary column is referenced in a count function.
Charts and tables depicting F1 and precision scores are available at the end of this chapter.

\paragraph{\textbf{Identifier-Level Linking Analysis}}

For an identifier-focused (rather than query-focused) metric, we perform identifier-level linking analysis.
We derive recall linking scores for each Native schema identifier $I$ as follows. 
$I_{match}$ is the count of instances when $I$ is correctly present in a predicted query. 
$I_{gold}$ is the count of gold queries that contain $I$.

\begin{equation}
  \label{eq:identifier-recall}
  IdentifierRecall = \frac{I_{match}}{I_{gold}}
\end{equation}

Figure \ref{fig:identifier-recall} visualizes \emph{IdentifierRecall} of Native identifiers in each naturalness level, and for each LLM.
The chart indicates an observable difference in \emph{IdentifierRecall} scores for each naturalness level, with \emph{IdentifierRecall} increasing for higher naturalness levels.
These results remain consistent relative to overall model performance across all 5 LLMs and various workflows.

\paragraph{\textbf{Naturalness Effect on Schema Linking}}

\begin{figure}
  \centering
  \includegraphics[width=\figwidthmod\linewidth]{figures/identifier-recall.pdf}
  \caption{Native identifier recall scores by model and naturalness level. Error bars set with confidence interval of 0.95. For all models, identifiers in lower naturalness categories yield lower recall scores.}
  \label{fig:identifier-recall}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=\figwidthmod\linewidth]{figures/recall-boxplot-db-combined.pdf}
  \caption{Schema linking performance across database schema naturalness levels generally yields equal or better performance for higher levels of naturalness, with open source models Phind-CodeLlama2 (Ph-CdLlm2-ZS) and CodeS as well as OpenAI's GPT-3.5 (GPT-3.5-ZS) exhibiting higher sensitivity to changes in naturalness. Zero-shot prompting NL-to-SQL methods are denoted as (ZS).}
  \label{fig:modelrecallnaturalnesslevels}
\end{figure}


Overall, we find that schema naturalness has a model-dependent and significant effect on schema linking performance with the highest correlations between \emph{QueryRecall} and query naturalness occurring with the open-source CodeLlama and CodeS models, and the lowest (though still significant) correlations occurring with Google's SoTA Gemini 1.5 Pro and OpenAI's GPT-4o models.
The more complex DIN SQL and CodeS workflow \emph{QueryRecall} results are also significantly affected by naturalness level differences.

Both DIN SQL and the CodeS complex NL-to-SQL workflows are sensitive to changes in naturalness, suggesting that these more complex workflows by themselves do not overcome schema naturalness effects.  
We also see that execution accuracy differences between the GPT-4o zero-shot prompting method and the DINSQL prompt chaining method suggest that applying more complex workflows to high-performing LLMs may be counterproductive for more recent SoTA LLMs.

Figure \ref{fig:modelrecallnaturalnesslevels} illustrates \emph{QueryRecall} across schema naturalness levels, and for each LLM.
For GPT 3.5, Phind-CodeLlama2, and CodeS, we observe an improvement to \emph{QueryRecall} when converting identifiers in a Native schema to Regular naturalness.
This improvement did not manifest for Gemini and GPT-4o when observing the data in aggregate (i.e., between databases) due to their overall high performance relative to the other models, but improvements within databases of lower naturalness are still present (see Figure \ref{fig:recall-boxplot-dbs}).
The recall drop (approximately 20 percent decrease) associated with a modification from both Regular and Low to Least naturalness remains consistent across all LLMs.

\begin{figure}
  \centering
  \begin{subfigure}{\figwidthmod\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figures/natlevel-model-recall-boxplot-db-subset-PILB.pdf}
  \end{subfigure}
  \begin{subfigure}{\figwidthmod\linewidth}
    \includegraphics[width=\linewidth]{figures/natlevel-model-recall-boxplot-db-subset-NTSB.pdf}
  \end{subfigure}
  \begin{subfigure}{\figwidthmod\linewidth}
    \includegraphics[width=\linewidth]{figures/natlevel-model-recall-boxplot-db-subset-SBOD.pdf}
  \end{subfigure}
  \caption{Schema linking performance (QueryRecall score) changes across 3 example databases' native and virtual schemas. We selected these 3 examples to showcase the diversity of the databases in our collection. PILB Native is a more natural schema with 65 percent Regular, 22 percent Low, and 13 percent Least; NTSB Native contains 42 percent Regular, 34 percent Low, and 24 percent Least; and SBOD Native is the lowest naturalness schema with 24 percent Regular, 49 percent Low, and 27 percent Least.}
  \label{fig:recall-boxplot-dbs}
\end{figure}

Naturalness changes within specific SNAILS database schemas paints a clearer picture of the impact of naturalness.
Figure \ref{fig:recall-boxplot-dbs} provides a drill-down view of the effect of schema modification on the PILB, SBOD, and NTSB schemas in terms of \emph{QueryRecall}, and for each LLM and schema naturalness level.
The first example (PILB) is a highly natural Native schema where schema naturalness modification would not be required. The center example (NTSB) indicates linking performance improvement across all models for a native schema of lower naturalness converted to a higher naturalness schema, and presents a case where naturalness modification will improve NLI performance. The bottom database (SBOD) represents a Least naturalness schema, and transformation from Native to Regular yields significant improvements for all models. In all cases, we see that reducing naturalness to the Least level consistently degrades \emph{QueryRecall}. 

\paragraph{\textbf{Statistical Significance}}
Kendall-Tau ($\tau$) Correlations between the proportion of Least identifiers and \emph{QueryRecall} range from $\tau=-0.16$ (Gemini) to $\tau=-0.28$ (Phind-CodeLlama2), with $P < 0.001$ for all models.
Both Regular and Low identifier proportions are significantly correlated with improved outcomes in terms of \emph{QueryRecall}.
Identifiers with Regular naturalness show the highest positive Kendall-Tau correlations ranging from $\tau=0.07$ (Gemini) to $\tau=0.20$ (Phind-CodeLlama2).
Low naturalness identifier proportions correlate positively, but to a lesser degree, with Kendall-Tau values ranging from $\tau=0.05$ (Phind-CodeLlama2) to $\tau=0.07$ (Gemini).

\begin{figure}
  \centering
  \includegraphics[width=\figwidthmod\linewidth]{figures/dinsql_codes_subsetting_performance.png}
  \caption{Schema subsetting performance, measured with recall, precision, and  f1 score, varies by naturalness levels for both DIN SQL and CodeS. Measurement Score is Recall, Precision, or f1 respectively.}
  \label{fig:subsettingperformance}
\end{figure}

Tables \ref{table:nathigh-recall-ktau-native}--\ref{table:natlow-manual-match-ktau-all} indicate that proportions of a naturalness category within a set of query identifiers also have an effect on linking performance.
Specifically, as the proportion of Regular naturalness identifiers increases, so does schema linking.
We also observe that in most cases, as the proportion of Low increases, schema linking generally improves, but not to the same degree as for Regular identifiers.
The most striking effect comes from the proportion of Least identifiers, where as the proportion of Least naturalness increases, schema linking performance decreases. 

\paragraph{\textbf{Naturalness Effects on Schema Subsetting}}
We measure the schema subsetting (also known as schema filtering, or table retrieval) in terms of recall, precision, and f1 score, and present the results in Figure \ref{fig:subsettingperformance}.
We find that for the CodeS finetuned classifier approach, schema naturalness level differences result in observable differences in f1.
For the DIN SQL LLM-based approach, naturalness effects are less pronounced, though still present, particularly for Least level schemas.

\paragraph{\textbf{Performance Over Modified Spider Schemas}}

\begin{figure}
  \centering
  \includegraphics[width=\figwidthmod\linewidth]{figures/spider-combined-results.pdf}
  \caption{QueryRecall and Execution Accuracy differences over the Spider~\cite{Yu&al.18c} dev set modified using SNAILS renaming artifacts.}
  \label{fig:spiderresults}
\end{figure}

Figure \ref{fig:spiderresults} shows that with the SNAILS schema renaming artifacts applied to the Spider NL-to-SQL benchmark dev dataset~\cite{Yu&al.18c}, naturalness effects are the most significant between Low and Least levels of naturalness.
Performance differences across naturalness levels for the highly natural Spider schemas resemble performance over similarly-natural schemas in the SNAILS collection.



\section{Discussion and Limitations}


The ability to assess the naturalness of existing schemas can inform the feasibility of ``hooking up'' an NL query interface to an existing database.
We believe that practitioners who are considering the integration of an LLM into their database interaction workflows would benefit from making naturalness-focused schema analysis a key step in their integration process.

\paragraph{\textbf{Other Naming Patterns in Real-World Schemas}}
To examine naming practices in the real-world, we classified the identifiers of SchemaPile dataset~\cite{doehmen2024schemapile} with our CANINE-based classifier, and evaluated the identifiers for other LLM-unfriendly patterns.
We observe that whitespace characters within schema identifiers contributes to identifier mutation during inference.
That is, rather than encasing a whitespace-containing identifier with brackets or quotes, the LLM hallucinates the identifier into snake or camel case format.
We find that in the SchemaPile collection, though whitespace is uncommon (less than 1 percent for both tables and columns), it appears in 808 columns and 63 tables, and is comparable to the proportions in the SNAILS dataset.

Another naming practice that yields disproportionate failures with some LLMs is the presence of the word \emph{table} in the identifier name.
In these instances, we find that the LLM tends to drop the word \emph{table} from the name (e.g., table\_employee becomes employee).
There are over 700 identifiers (less than 1 percent of all identifiers) in the SchemaPile collection that employ this naming pattern.

These observations suggest that although these naming patterns are not necessarily a common occurrence in many real-world schema designs, they do appear in some cases. 
We suggest that practitioners would benefit from assessing the naming patterns of their database schemas.


\paragraph{\textbf{Variations in LLM Sensitivity to Naturalness}}
There are many LLMs to select from for NLIDBs, and we can see even within the select 5 models in our work large variations in NL-to-SQL performance as well as the degree of sensitivity to schema naturalness.
The Google Gemini and GPT-4o models demonstrate the highest overall performance, as well as the lowest sensitivity to naturalness differences between Regular and Low levels.
Without access to the underlying model architectures and weights, it remains as a black box in our research, and we can merely speculate the reasons why it is not as affected by naturalness as the other 3 models in our study.
Generally, we observe that the these models have an overall higher performance, and are less prone to linking errors such as selecting the incorrect identifier from the schema knowledge representation or committing a typo-like hallucination.

Though selecting the most performant model would seem to be an obvious course of action, competing factors such as an organization's policies, budget, or existing vendor contracts, may require the selection of a model that is more sensitive to schema naturalness differences. 
Thus, we believe that naturalness-aware NLI integration will remain important for at least the practitioners who use LLMs other than Gemini in the set that we have studied.


\paragraph{\textbf{Modifying Existing Schemas}}
For already-existing schemas, renaming identifiers is generally a non-trivial effort, particularly for those databases for which documentation has been published and application interfaces have been integrated.
Schema modifications may not be necessary (or helpful), if a schema is already classified as highly natural.
DBAs should assess current naturalness levels prior to committing to naming modifications.
At a minimum, we recommend that any Least identifier be modified to a Regular naturalness level and, if feasible, Low identifiers as well.
If renaming a less natural schema's identifiers is not feasible due to integration constraints, we suggest one of two approaches: 1) adopting a naturalness-as-a-view strategy by mapping Native identifiers to Regular naturalness identifiers using SQL views, or 2) a middleware approach that modifies schema knowledge in LLM prompts and generated SQL queries prior to execution on the database.


We demonstrate a natural schema view proof of concept with our SNAILS database collection and their MS SQL Server instances.
For each table and column in the collection's database schemas, we map the Native table or column to its Regular counterpart in the naturalness modified identifier dataset using SQL view creation DDL and a db\_nl schema.
This enables schema information retrieval for LLM-based NL-to-SQL prompting without prompt or generated query modification while still retaining the underlying Native schema naming patterns required for existing integrations.

In lieu of schema modificaftion, practitioners may elect to employ prompting techniques that augment schema representations with additional metadata or value samples.
While these methods may improve schema linking performance in some contexts~\cite{nan2023enhancingfewshottexttosqlcapabilities}, they greatly increase schema representations on a per-identifier basis.
Thus, the cost to do so is high in terms of token efficiency, latency, and implementation complexity, especially for very large schemas.

\paragraph{\textbf{Designing New Schemas}}
For new schema development, our results show that making schema identifiers more natural from the start can make databases work better with LLMs.
Specifically, database designers should try to avoid Least naturalness identifiers and would likely also benefit from limiting Low naturalness identifiers.
Database practitioners can evaluate the naturalness of identifiers using the identifier naturalness classification techniques and model artifacts described in this paper and released publicly by us as part of the SNAILS collection.

\paragraph{\textbf{Limitations}}
LLM research is advancing rapidly, and the LLMs represented in this paper may get superseded by newer versions or newer models (e.g., DBRX~\cite{dbrx}, Arctic~\cite{snowflakearctic}).
But it does not negate our work's core value--the first in-depth characterization of how schema naturalness affects LLM-based NL-to-SQL--and our new labeled datasets, AI artifacts, and benchmarking framework can be used for future LLMs too.
We leave it to future work to also include such very recent LLMs for further benchmark analyses.

We recognize that the correlation statistics indicate a moderate (in some cases only a weak) correlation between naturalness and \emph{IdentifierRecall}.
This suggests that other undiscovered factors also influence linking performance; and further research may reveal additional schema- and language-related correlations.

Our selection of 9 database schemas is of course not fully representative of \emph{all} types of schemas available in the real-world.
The SNAILS collection will benefit from continued growth in terms of both databases and NL-SQL pairs.
We hope our open source datasets and artifacts can be built upon by the database and NLP communities to keep improving LLM-based NL-to-SQL.

%BEGIN-CONCLUSIONS
%END-CONCLUSIONS

%BEGIN-FUTURE-WORK
%END-FUTURE-WORK


\section{Practical Applications}

It is clear that naturalness has an effect on multiple NL-to-SQL performance measurements, but what is less clear is what should be done about it.
Adopting good schema naming practices, including the use of natural words, can be easily applied when designing \emph{new} schemas, which makes the application of naturalness-based performance improvements relatively straightforward in these cases.
For existing schemas, the challenge is much greater, as it is likely that external interfaces and documentation have coalesced around the database schema, making it difficult (or impossible) to change without overhauling external systems and artifacts.

\subsection{For \emph{New} Databases}
We refer the reader to Section \ref{section:naturalness-categories} for the descriptions of Regular, Low, and Least category criteria. 
Additionally, Table \ref{table:nat-cat-examples} provides some examples of database identifiers at each naturalness level.

When creating a new database schema, we recommend that designers apply the Regular definition criteria, where the identifier contains complete English words with no abbreviations or acronyms, or contains only acronyms in common usage.
We also recommend avoiding the use of whitespace characters, as well as identifier type labels (e.g., \emph{table} or \emph{column}), as we observe that some LLMs tend to drop these words during NL-to-SQL inference.

\subsection{For \emph{Existing} Databases}

Modifying existing database schemas directly is infeasible for a myriad of reasons ranging from external integrations to constraint management within the database.
As such, we offer two viable approaches to making database interactions more natural: 1) schema and query modification middleware, and 2) a within-database natural view.

\paragraph{\textbf{Schema Modification Middleware}}
This approach is the more complex of the two, but may be necessary in cases where practicioners do not have write access to the target database.
This approach contains the following pre-processing steps:
\begin{enumerate}
  \item Classify schema identifiers using \emph{Artifact 3}.
  \item For Low and Least identifiers classified in step 1: create Regular representations using \emph{Artifact 5}.
  \item Create a Native-to-Regular map (or crosswalk) for all identifiers using the output of step 2 (see \emph{Artifact 4} for an example).
\end{enumerate}

The output of the preprocessing steps is the Native-to-Regular map that maps every Native  database identifier to a Regular representation.
In the case where the Native identifier has a Regular classification, it should be mapped to itself.

The next step involves the development of a middleware that modifies schema knowledge during NL-to-SQL inference so that the LLM receives a Regular naturalness schema representation.
It contains the following steps:
\begin{enumerate}
  \item Modify prompt schema knowledge by replacing Low and Least identifiers with Regular representations drawn from the identifier map.
  \item Incorporate modified schema knowledge into the NL-to-SQL workflows involving schema representations (e.g., schema filtering and SQL generation steps).
  \item After SQL generation, modify the SQL query by replacing Regular naturalness representations of lower naturalness Native identifiers to enable compatibility with the Native schema.
\end{enumerate}

The output of the inference-time steps is a SQL query that can be executed over the target Native database.

The SNAILS project repository contains a prototype of such a middleware system, which is incorporated into the NL-to-SQL workflow used in our experimental design.
\\The \emph{nl\_to\_sql\_inference\_and\_prompt\_generation.py} file employs the \emph{naturalize\_prompt()} and \emph{denaturalize\_query()} functions to enable NL-to-SQL inference over natural schemas.
While this is not an easily portable and standalone system, we encourage interested readers to trace the processes in these scripts for an example of a middleware solution.

Schema naturalization for LLM prompting is a fairly straightforward map lookup task. 
On the other hand, query ``denaturalization'' presents a more technical challenge due to the large variety of SQL queries that can be generated for a given NL question.
To consistently replace identifiers in SQL queries, we create a Java-based SQL parser that supports both Sqlite and T-SQL syntax.
This parser and query analyzer provides two important services: 1) query clause extraction, which we use for measuring query complexity; and 2) schema identifier tagging, which we use for query denaturalization.
The latter feature (tagging) takes a SQL query as input, and returns the same query where all table and column names are encased within XML-like tags (e.g., <TABLE\_NAME>\emph{Customers}</TABLE\_NAME>).
We discuss this in more detail in Section \ref{section:querynaturalnessmodification}.

\paragraph{\textbf{Natural Views}}

The natural view concept is simple, but also very powerful.
Rather than incorporating a relatively complex middleware strategy, for databases that support multiple schemas within an instance such as MS Sql Server we can create views that map a Regular naturalness representation of tables and their columns to their Native identifier counterparts within the base database schema.
This approach is suitable when 2 main criteria can be met: 1) the user has schema and view creation privileges, and 2) the database supports multiple schemas for a database instance (e.g., a base \emph{dbo} schema and a natural \emph{db\_nl} schema).
Seperate schemas are required to avoid collisions between a natural view and a native schema where the native schema tables already have Regular naturalness levels.

The SNAILS project repository contains a prototype end-to-end natural view creation example that generates natural views for the SNAILS databases in a \emph{db\_nl} schema, which can be viewed and used by downloading the SNAILS real-world database collection (\emph{artifact 1}).
The \emph{classify\_rename\_and\_build\_view.py} demonstrates the process of schema classification, identifier modification, and view creation over a target MS SQL Server database.

We first begin with the same pre-processing steps as the middleware approach where we:
\begin{enumerate}
  \item Classify schema identifiers using \emph{Artifact 3}.
  \item For Low and Least identifiers classified in step 1: create Regular representations using \emph{Artifact 5}.
  \item Create a Native-to-Regular map (or crosswalk) for all identifiers using the output of step 2 (see \emph{Artifact 4} for an example).
\end{enumerate}

At this point, with the naturalness map (or crosswalk) as input, we generate a set of SQL view creation queries--one for each table in the schema.
The resulting view query appears as follows:

\begin{verbatim}
  CREATE VIEW db_nl.[table_deadwood] AS
    SELECT 
      [Data_ID] AS [Data_ID], 
      [Event_ID] AS [Event_ID], 
      [OldPlot] AS [OldPlot], 
      [Module] AS [Module], 
      [Decay] AS [Decay], 
      [MPD] AS [Midpoint_Diameter], 
      [Length] AS [Length], 
      [X_coord] AS [x_coordinate], 
      [Y_coord] AS [y_coordinate]
    FROM dbo.[tbl_Deadwood];
\end{verbatim}

We make note of a few important aspects of the natural view: 1) Many identifiers map to themselves, as their Native naturalness is already Regular. 2) to avoid table name collisions, the views are mapped from the \emph{dbo} schema to the \emph{db\_nl} schema. 3) This particular transformation contains an example of a poor naming habit (the word \emph{table} in \emph{table\_deadwood}), and serves to remind us that we should typically review the output of the schema renamer and make necessary changes.


\section{Additional Tables and Figures}


The remaining pages contain several figures and tables of fine-grained analysis of dataset distributions and performance correlations.

\paragraph{Kendall-Tau Correlation Experiment Result Tables}

Tables \ref{table:tokenratio-recall-ktau-native}-\ref{table:executionktauall} provide Kendall-Tau experiment results for naturalness and token ratio correlations with linking performance (f1, recall, and precision) and execution accuracy.

\begin{itemize}
  \item Table \ref{table:tokenratio-recall-ktau-native}: Token-Character ratio--Recall
  \item Tables \ref{table:recallktaunative}--\ref{table:precisionktauall}: Combined naturalness--[Recall, f1, Precision]
  \item Tables \ref{table:nathigh-recall-ktau-native}--\ref{table:natlow-recall-ktau-all}: [Regular, Low, Least] naturalness--recall
  \item Tables \ref{table:nathigh-f1-ktau-native}--\ref{table:natlow-f1-ktau-all}: [Regular, Low, Least] naturalness--f1
  \item Tables \ref{table:nathigh-precision-ktau-native}--\ref{table:natlow-precision-ktau-all}: [Regular, Low, Least] naturalness--precision
  \item Tables \ref{table:nathigh-manual-match-ktau-native}--\ref{table:executionktauall}: [Combined, Regular, Low, Least] naturalness--execution accuracy
\end{itemize}

\paragraph{Database-level Schema Linking Box and Whisker Plots}

Figures \ref{fig:db-level-boxplots-f1-1}-\ref{fig:db-level-boxplots-recall-2} show additional database-level box and whisker plots depicting schema linking performance over individual database schemas and their naturalness levels for each LLM.



% Mean token char ratio K tau tables for Recall

\begin{table*}[ht]
  \centering
  \caption{Kendall-Tau ($\tau$) Correlations between the \emph{Mean Token-to-Character Ratio} and \emph{Query Recall}.}
  \begin{subfigure}{.5\linewidth}
      \centering
      \caption{Native schemas}
      \begin{tabular}{lllr}
\toprule
Model & $\tau$ & p & n \\
\midrule
Gem-1.5-ZS & -0.147 & 0.000 & 503 \\
GPT-4o-ZS & -0.126 & 0.000 & 513 \\
DINSQL & -0.156 & 0.000 & 503 \\
GPT-3.5-ZS & -0.254 & 0.000 & 503 \\
Ph-CdLlm2-ZS & -0.273 & 0.000 & 503 \\
CodeS & -0.221 & 0.000 & 503 \\
\bottomrule
\end{tabular}

      \label{table:tokenratio-recall-ktau-native}
  \end{subfigure}%
  \begin{subfigure}{.5\linewidth}
      \centering
      \caption{All schemas (native + modified)}
      \begin{tabular}{lllr}
\toprule
Model & $\tau$ & p & n \\
\midrule
Gem-1.5-ZS & -0.132 & 0.000 & 2012 \\
GPT-4o-ZS & -0.139 & 0.000 & 2022 \\
DINSQL & -0.133 & 0.000 & 2012 \\
GPT-3.5-ZS & -0.167 & 0.000 & 2012 \\
Ph-CdLlm2-ZS & -0.255 & 0.000 & 2012 \\
CodeS & -0.270 & 0.000 & 2012 \\
\bottomrule
\end{tabular}

      \label{table:tokenratio-recall-ktau-all}
  \end{subfigure}
\end{table*}

\begin{table*}
  \centering
  \caption{Kendall-Tau ($\tau$) Correlations between \emph{Query Combined Naturalness} and \emph{Query Recall}.}
  \begin{subfigure}{.5\linewidth}
    % \begin{table}
      \centering
      \caption{Native schemas}
      \begin{tabular}{lllr}
\toprule
Model & $\tau$ & p & n \\
\midrule
Gem-1.5-ZS & 0.138 & 0.000 & 492 \\
GPT-4o-ZS & 0.146 & 0.000 & 512 \\
DINSQL & 0.183 & 0.000 & 503 \\
GPT-3.5-ZS & 0.209 & 0.000 & 500 \\
Ph-CdLlm2-ZS & 0.254 & 0.000 & 484 \\
CodeS & 0.199 & 0.000 & 501 \\
\bottomrule
\end{tabular}

      \label{table:recallktaunative}
    % \end{table}
  \end{subfigure}%
  \begin{subfigure}{.5\linewidth}
    % \begin{table}
      \centering
      \caption{All schemas (native + modified)}
      \begin{tabular}{lllr}
\toprule
Model & $\tau$ & p & n \\
\midrule
Gem-1.5-ZS & 0.114 & 0.000 & 1978 \\
GPT-4o-ZS & 0.154 & 0.000 & 2009 \\
DINSQL & 0.152 & 0.000 & 2007 \\
GPT-3.5-ZS & 0.172 & 0.000 & 1998 \\
Ph-CdLlm2-ZS & 0.250 & 0.000 & 1936 \\
CodeS & 0.286 & 0.000 & 2008 \\
\bottomrule
\end{tabular}

      \label{table:recallktauall}
    % \end{table}
  \end{subfigure}
\end{table*}

\begin{table*}
  \centering
  \caption{Kendall-Tau ($\tau$) Correlations between \emph{Query Combined Naturalness} and \emph{Query f1}.}
  \begin{subfigure}{.5\linewidth}
    % \begin{table}
      \centering
      \caption{Native schemas}
      \begin{tabular}{lllr}
\toprule
Model & $\tau$ & p & n \\
\midrule
Gem-1.5-ZS & 0.151 & 0.000 & 492 \\
GPT-4o-ZS & 0.130 & 0.000 & 512 \\
DINSQL & 0.137 & 0.000 & 503 \\
GPT-3.5-ZS & 0.214 & 0.000 & 500 \\
Ph-CdLlm2-ZS & 0.253 & 0.000 & 484 \\
CodeS & 0.216 & 0.000 & 501 \\
\bottomrule
\end{tabular}

      \label{table:f1ktaunative}
    % \end{table}
  \end{subfigure}%
  \begin{subfigure}{.5\linewidth}
    % \begin{table}
      \centering
      \caption{All schemas (native + modified)}
      \begin{tabular}{lllr}
\toprule
Model & $\tau$ & p & n \\
\midrule
Gem-1.5-ZS & 0.110 & 0.000 & 1978 \\
GPT-4o-ZS & 0.142 & 0.000 & 2009 \\
DINSQL & 0.147 & 0.000 & 2007 \\
GPT-3.5-ZS & 0.185 & 0.000 & 1998 \\
Ph-CdLlm2-ZS & 0.249 & 0.000 & 1936 \\
CodeS & 0.286 & 0.000 & 2008 \\
\bottomrule
\end{tabular}

      \label{table:f1ktauall}
    % \end{table}
  \end{subfigure}
  
\end{table*}

\begin{table*}
  \centering
  \caption{Kendall-Tau ($\tau$) Correlations between \emph{Query Combined Naturalness} and \emph{Query Precision}.}
  \begin{subfigure}{.5\linewidth}
    % \begin{table}
      \centering
      \caption{Native schemas}
      \begin{tabular}{lllr}
\toprule
Model & $\tau$ & p & n \\
\midrule
Gem-1.5-ZS & 0.166 & 0.000 & 492 \\
GPT-4o-ZS & 0.117 & 0.000 & 512 \\
DINSQL & 0.084 & 0.012 & 503 \\
GPT-3.5-ZS & 0.214 & 0.000 & 500 \\
Ph-CdLlm2-ZS & 0.241 & 0.000 & 484 \\
CodeS & 0.223 & 0.000 & 501 \\
\bottomrule
\end{tabular}

      \label{table:precisionktaunative}
    % \end{table}
  \end{subfigure}%
  \begin{subfigure}{.5\linewidth}
    % \begin{table}
      \centering
      \caption{All schemas (native + modified)}
      \begin{tabular}{lllr}
\toprule
Model & $\tau$ & p & n \\
\midrule
Gem-1.5-ZS & 0.100 & 0.000 & 1978 \\
GPT-4o-ZS & 0.130 & 0.000 & 2009 \\
DINSQL & 0.149 & 0.000 & 2007 \\
GPT-3.5-ZS & 0.194 & 0.000 & 1998 \\
Ph-CdLlm2-ZS & 0.240 & 0.000 & 1936 \\
CodeS & 0.279 & 0.000 & 2008 \\
\bottomrule
\end{tabular}

      \label{table:precisionktauall}
    % \end{table}
  \end{subfigure}
  
\end{table*}

% Naturalness proportion K tau tables for recall:

\begin{table*}
  \centering
  \caption{Kendall-Tau ($\tau$) Correlations between \emph{Regular Identifier Proportion} and \emph{Query Recall}.}
  \begin{subfigure}{.5\linewidth}
    % \begin{table}
      \centering
      \caption{Native schemas}
      \begin{tabular}{lllr}
\toprule
Model & $\tau$ & p & n \\
\midrule
Gem-1.5-ZS & 0.109 & 0.002 & 503 \\
GPT-4o-ZS & 0.140 & 0.000 & 513 \\
DINSQL & 0.163 & 0.000 & 503 \\
GPT-3.5-ZS & 0.180 & 0.000 & 503 \\
Ph-CdLlm2-ZS & 0.209 & 0.000 & 503 \\
CodeS & 0.158 & 0.000 & 503 \\
\bottomrule
\end{tabular}

      \label{table:nathigh-recall-ktau-native}
    % \end{table}
  \end{subfigure}%
  \begin{subfigure}{.5\linewidth}
    % \begin{table}
      \centering
      \caption{All schemas (native + modified)}
      \begin{tabular}{lllr}
\toprule
Model & $\tau$ & p & n \\
\midrule
Gem-1.5-ZS & 0.065 & 0.000 & 2012 \\
GPT-4o-ZS & 0.117 & 0.000 & 2022 \\
DINSQL & 0.102 & 0.000 & 2012 \\
GPT-3.5-ZS & 0.121 & 0.000 & 2012 \\
Ph-CdLlm2-ZS & 0.187 & 0.000 & 2012 \\
CodeS & 0.227 & 0.000 & 2012 \\
\bottomrule
\end{tabular}

      \label{table:nathigh-recall-ktau-all}
    % \end{table}
  \end{subfigure}
\end{table*}

\begin{table*}
  \centering
  \caption{Kendall-Tau ($\tau$) Correlations between \emph{Low Identifier Proportion} and \emph{Query Recall}.}
  \begin{subfigure}{.5\linewidth}
    % \begin{table}
      \centering
      \caption{Native schemas}
      \begin{tabular}{lllr}
\toprule
Model & $\tau$ & p & n \\
\midrule
Gem-1.5-ZS & -0.007 & 0.848 & 503 \\
GPT-4o-ZS & -0.037 & 0.292 & 513 \\
DINSQL & -0.028 & 0.425 & 503 \\
GPT-3.5-ZS & -0.015 & 0.648 & 503 \\
Ph-CdLlm2-ZS & -0.024 & 0.460 & 503 \\
CodeS & 0.025 & 0.462 & 503 \\
\bottomrule
\end{tabular}

      \label{table:natmedium-recall-ktau-native}
    % \end{table}
  \end{subfigure}%
  \begin{subfigure}{.5\linewidth}
    % \begin{table}
      \centering
      \caption{All schemas (native + modified)}
      \begin{tabular}{lllr}
\toprule
Model & $\tau$ & p & n \\
\midrule
Gem-1.5-ZS & 0.071 & 0.000 & 2012 \\
GPT-4o-ZS & 0.044 & 0.021 & 2022 \\
DINSQL & 0.084 & 0.000 & 2012 \\
GPT-3.5-ZS & 0.075 & 0.000 & 2012 \\
Ph-CdLlm2-ZS & 0.055 & 0.002 & 2012 \\
CodeS & 0.073 & 0.000 & 2012 \\
\bottomrule
\end{tabular}

      \label{table:natmedium-recall-ktau-all}
    % \end{table}
  \end{subfigure}

\end{table*}

\begin{table*}
  \centering
  \caption{Kendall-Tau ($\tau$) Correlations between \emph{Least Identifier Proportion} and \emph{Query Recall}.}
  \begin{subfigure}{.5\linewidth}
    % \begin{table}
      \centering
      \caption{Native schemas}
      \begin{tabular}{lllr}
\toprule
Model & $\tau$ & p & n \\
\midrule
Gem-1.5-ZS & -0.174 & 0.000 & 503 \\
GPT-4o-ZS & -0.151 & 0.000 & 513 \\
DINSQL & -0.207 & 0.000 & 503 \\
GPT-3.5-ZS & -0.237 & 0.000 & 503 \\
Ph-CdLlm2-ZS & -0.289 & 0.000 & 503 \\
CodeS & -0.260 & 0.000 & 503 \\
\bottomrule
\end{tabular}

      \label{table:natlow-recall-ktau-native}
    % \end{table}
  \end{subfigure}%
  \begin{subfigure}{.5\linewidth}
    % \begin{table}
      \centering
      \caption{All schemas (native + modified)}
      \begin{tabular}{lllr}
\toprule
Model & $\tau$ & p & n \\
\midrule
Gem-1.5-ZS & -0.156 & 0.000 & 2012 \\
GPT-4o-ZS & -0.175 & 0.000 & 2022 \\
DINSQL & -0.200 & 0.000 & 2012 \\
GPT-3.5-ZS & -0.212 & 0.000 & 2012 \\
Ph-CdLlm2-ZS & -0.270 & 0.000 & 2012 \\
CodeS & -0.310 & 0.000 & 2012 \\
\bottomrule
\end{tabular}

      \label{table:natlow-recall-ktau-all}
    % \end{table}
  \end{subfigure}
\end{table*}




% Naturalness proportion K tau tables for F1:

\begin{table*}
  \centering
  \caption{Kendall-Tau ($\tau$) Correlations between \emph{Regular Identifier Proportion} and \emph{Query f1}.}
  \begin{subfigure}{.5\linewidth}
    % \begin{table}
      \centering
      \caption{Native schemas}
      \begin{tabular}{lllr}
\toprule
Model & $\tau$ & p & n \\
\midrule
Gem-1.5-ZS & 0.123 & 0.000 & 503 \\
GPT-4o-ZS & 0.140 & 0.000 & 513 \\
DINSQL & 0.124 & 0.000 & 503 \\
GPT-3.5-ZS & 0.183 & 0.000 & 503 \\
Ph-CdLlm2-ZS & 0.221 & 0.000 & 503 \\
CodeS & 0.175 & 0.000 & 503 \\
\bottomrule
\end{tabular}

      \label{table:nathigh-f1-ktau-native}
    % \end{table}
  \end{subfigure}%
  \begin{subfigure}{.5\linewidth}
    % \begin{table}
      \centering
      \caption{All schemas (native + modified)}
      \begin{tabular}{lllr}
\toprule
Model & $\tau$ & p & n \\
\midrule
Gem-1.5-ZS & 0.067 & 0.000 & 2012 \\
GPT-4o-ZS & 0.112 & 0.000 & 2022 \\
DINSQL & 0.107 & 0.000 & 2012 \\
GPT-3.5-ZS & 0.135 & 0.000 & 2012 \\
Ph-CdLlm2-ZS & 0.191 & 0.000 & 2012 \\
CodeS & 0.230 & 0.000 & 2012 \\
\bottomrule
\end{tabular}

      \label{table:nathigh-f1-ktau-all}
    % \end{table}
  \end{subfigure}
\end{table*}

\begin{table*}
  \centering
  \caption{Kendall-Tau ($\tau$) Correlations between \emph{Low Identifier Proportion} and \emph{Query f1}.}
  \begin{subfigure}{.5\linewidth}
    % \begin{table}
      \centering
      \caption{Native schemas}
      \begin{tabular}{lllr}
\toprule
Model & $\tau$ & p & n \\
\midrule
Gem-1.5-ZS & -0.014 & 0.662 & 503 \\
GPT-4o-ZS & -0.066 & 0.044 & 513 \\
DINSQL & -0.021 & 0.517 & 503 \\
GPT-3.5-ZS & -0.018 & 0.573 & 503 \\
Ph-CdLlm2-ZS & -0.053 & 0.098 & 503 \\
CodeS & 0.014 & 0.667 & 503 \\
\bottomrule
\end{tabular}

      \label{table:natmedium-f1-ktau-native}
    % \end{table}
  \end{subfigure}%
  \begin{subfigure}{.5\linewidth}
    % \begin{table}
      \centering
      \caption{All schemas (native + modified)}
      \begin{tabular}{lllr}
\toprule
Model & $\tau$ & p & n \\
\midrule
Gem-1.5-ZS & 0.061 & 0.001 & 2012 \\
GPT-4o-ZS & 0.037 & 0.042 & 2022 \\
DINSQL & 0.068 & 0.000 & 2012 \\
GPT-3.5-ZS & 0.068 & 0.000 & 2012 \\
Ph-CdLlm2-ZS & 0.045 & 0.009 & 2012 \\
CodeS & 0.075 & 0.000 & 2012 \\
\bottomrule
\end{tabular}

      \label{table:natmedium-f1-ktau-all}
    % \end{table}
  \end{subfigure}
\end{table*}

\begin{table*}
  \centering
  \caption{Kendall-Tau ($\tau$) Correlations between \emph{Least Identifier Proportion} and \emph{Query f1}.}
  \begin{subfigure}{.5\linewidth}
    % \begin{table}
      \centering
      \caption{Native schemas}
      \begin{tabular}{lllr}
\toprule
Model & $\tau$ & p & n \\
\midrule
Gem-1.5-ZS & -0.167 & 0.000 & 503 \\
GPT-4o-ZS & -0.093 & 0.008 & 513 \\
DINSQL & -0.136 & 0.000 & 503 \\
GPT-3.5-ZS & -0.242 & 0.000 & 503 \\
Ph-CdLlm2-ZS & -0.247 & 0.000 & 503 \\
CodeS & -0.249 & 0.000 & 503 \\
\bottomrule
\end{tabular}

      \label{table:natlow-f1-ktau-native}
    % \end{table}
  \end{subfigure}%
  \begin{subfigure}{.5\linewidth}
    % \begin{table}
      \centering
      \caption{All schemas (native + modified)}
      \begin{tabular}{lllr}
\toprule
Model & $\tau$ & p & n \\
\midrule
Gem-1.5-ZS & -0.145 & 0.000 & 2012 \\
GPT-4o-ZS & -0.150 & 0.000 & 2022 \\
DINSQL & -0.176 & 0.000 & 2012 \\
GPT-3.5-ZS & -0.220 & 0.000 & 2012 \\
Ph-CdLlm2-ZS & -0.256 & 0.000 & 2012 \\
CodeS & -0.305 & 0.000 & 2012 \\
\bottomrule
\end{tabular}

      \label{table:natlow-f1-ktau-all}
    % \end{table}
  \end{subfigure}
\end{table*}

% Naturalness proportion K tau tables for precision:

\begin{table*}
  \centering
  \caption{Kendall-Tau ($\tau$) Correlations between \emph{Regular Identifier Proportion} and \emph{Query Precision}.}
  \begin{subfigure}{.5\linewidth}
    % \begin{table}
      \centering
      \caption{Native schemas}
      \begin{tabular}{lllr}
\toprule
Model & $\tau$ & p & n \\
\midrule
Gem-1.5-ZS & 0.131 & 0.000 & 503 \\
GPT-4o-ZS & 0.119 & 0.000 & 513 \\
DINSQL & 0.070 & 0.039 & 503 \\
GPT-3.5-ZS & 0.177 & 0.000 & 503 \\
Ph-CdLlm2-ZS & 0.214 & 0.000 & 503 \\
CodeS & 0.177 & 0.000 & 503 \\
\bottomrule
\end{tabular}

      \label{table:nathigh-precision-ktau-native}
    % \end{table}
  \end{subfigure}%
  \begin{subfigure}{.5\linewidth}
    % \begin{table}
      \centering
      \caption{All schemas (native + modified)}
      \begin{tabular}{lllr}
\toprule
Model & $\tau$ & p & n \\
\midrule
Gem-1.5-ZS & 0.060 & 0.001 & 2012 \\
GPT-4o-ZS & 0.106 & 0.000 & 2022 \\
DINSQL & 0.113 & 0.000 & 2012 \\
GPT-3.5-ZS & 0.146 & 0.000 & 2012 \\
Ph-CdLlm2-ZS & 0.186 & 0.000 & 2012 \\
CodeS & 0.224 & 0.000 & 2012 \\
\bottomrule
\end{tabular}

      \label{table:nathigh-precision-ktau-all}
    % \end{table}
  \end{subfigure}
\end{table*}

\begin{table*}
  \centering
  \caption{Kendall-Tau ($\tau$) Correlations between \emph{Low Identifier Proportion} and \emph{Query Precision}.}
  \begin{subfigure}{.5\linewidth}
    % \begin{table}
      \centering
      \caption{Native schemas}
      \begin{tabular}{lllr}
\toprule
Model & $\tau$ & p & n \\
\midrule
Gem-1.5-ZS & -0.012 & 0.715 & 503 \\
GPT-4o-ZS & -0.043 & 0.205 & 513 \\
DINSQL & 0.014 & 0.679 & 503 \\
GPT-3.5-ZS & 0.001 & 0.985 & 503 \\
Ph-CdLlm2-ZS & -0.052 & 0.106 & 503 \\
CodeS & 0.033 & 0.324 & 503 \\
\bottomrule
\end{tabular}

      \label{table:natmedium-precision-ktau-native}
    % \end{table}
  \end{subfigure}%
  \begin{subfigure}{.5\linewidth}
    % \begin{table}
      \centering
      \caption{All schemas (native + modified)}
      \begin{tabular}{lllr}
\toprule
Model & $\tau$ & p & n \\
\midrule
Gem-1.5-ZS & 0.053 & 0.004 & 2012 \\
GPT-4o-ZS & 0.034 & 0.065 & 2022 \\
DINSQL & 0.067 & 0.000 & 2012 \\
GPT-3.5-ZS & 0.058 & 0.002 & 2012 \\
Ph-CdLlm2-ZS & 0.043 & 0.016 & 2012 \\
CodeS & 0.084 & 0.000 & 2012 \\
\bottomrule
\end{tabular}

      \label{table:natmedium-precision-ktau-all}
    % \end{table}
  \end{subfigure}
\end{table*}

\begin{table*}
  \centering
  \caption{Kendall-Tau ($\tau$) Correlations between \emph{Least Identifier Proportion} and \emph{Query Precision}.}
  \begin{subfigure}{.5\linewidth}
    % \begin{table}
      \centering
      \caption{Native schemas}
      \begin{tabular}{lllr}
\toprule
Model & $\tau$ & p & n \\
\midrule
Gem-1.5-ZS & -0.178 & 0.000 & 503 \\
GPT-4o-ZS & -0.094 & 0.009 & 513 \\
DINSQL & -0.094 & 0.010 & 503 \\
GPT-3.5-ZS & -0.247 & 0.000 & 503 \\
Ph-CdLlm2-ZS & -0.225 & 0.000 & 503 \\
CodeS & -0.254 & 0.000 & 503 \\
\bottomrule
\end{tabular}

      \label{table:natlow-precision-ktau-native}
    % \end{table}
  \end{subfigure}%
  \begin{subfigure}{.5\linewidth}
    % \begin{table}
      \centering
      \caption{All schemas (native + modified)}
      \begin{tabular}{lllr}
\toprule
Model & $\tau$ & p & n \\
\midrule
Gem-1.5-ZS & -0.134 & 0.000 & 2012 \\
GPT-4o-ZS & -0.138 & 0.000 & 2022 \\
DINSQL & -0.172 & 0.000 & 2012 \\
GPT-3.5-ZS & -0.222 & 0.000 & 2012 \\
Ph-CdLlm2-ZS & -0.242 & 0.000 & 2012 \\
CodeS & -0.299 & 0.000 & 2012 \\
\bottomrule
\end{tabular}

      \label{table:natlow-precision-ktau-all}
    % \end{table}
  \end{subfigure}
\end{table*}

% Naturalness proportion K tau tables for execution accuracy:

\begin{table*}
  \centering
  \caption{Kendall-Tau ($\tau$) Correlations between \emph{Regular Identifier Proportion} and \emph{Execution Accuracy}.}

  \begin{subfigure}{.5\linewidth}
    % \begin{table}
      \centering
      \caption{Native schemas}
      \begin{tabular}{lllr}
\toprule
Model & $\tau$ & p & n \\
\midrule
Gem-1.5-ZS & 0.032 & 0.402 & 503 \\
GPT-4o-ZS & 0.118 & 0.002 & 513 \\
DINSQL & 0.050 & 0.183 & 503 \\
GPT-3.5-ZS & 0.131 & 0.001 & 503 \\
Ph-CdLlm2-ZS & 0.134 & 0.000 & 503 \\
CodeS & 0.101 & 0.008 & 503 \\
\bottomrule
\end{tabular}

      \label{table:nathigh-manual-match-ktau-native}
    % \end{table}
  \end{subfigure}%
  \begin{subfigure}{.5\linewidth}
    % \begin{table}
      \centering
      \caption{All schemas (native + modified)}
      \begin{tabular}{lllr}
\toprule
Model & $\tau$ & p & n \\
\midrule
Gem-1.5-ZS & 0.051 & 0.014 & 2012 \\
GPT-4o-ZS & 0.118 & 0.000 & 2022 \\
DINSQL & 0.055 & 0.008 & 2012 \\
GPT-3.5-ZS & 0.114 & 0.000 & 2012 \\
Ph-CdLlm2-ZS & 0.157 & 0.000 & 2012 \\
CodeS & 0.154 & 0.000 & 2012 \\
\bottomrule
\end{tabular}

      \label{table:nathigh-manual-match-ktau-all}
    % \end{table}
  \end{subfigure}
\end{table*}

\begin{table*}
  \centering
  \caption{Kendall-Tau ($\tau$) Correlations between \emph{Low Identifier Proportion} and \emph{Execution Accuracy}.}
  \begin{subfigure}{.5\linewidth}
    % \begin{table}
      \centering
      \caption{Native schemas}
      \begin{tabular}{lllr}
\toprule
Model & $\tau$ & p & n \\
\midrule
Gem-1.5-ZS & 0.067 & 0.076 & 503 \\
GPT-4o-ZS & -0.050 & 0.178 & 513 \\
DINSQL & 0.006 & 0.881 & 503 \\
GPT-3.5-ZS & -0.014 & 0.711 & 503 \\
Ph-CdLlm2-ZS & -0.051 & 0.174 & 503 \\
CodeS & 0.007 & 0.852 & 503 \\
\bottomrule
\end{tabular}

      \label{table:natmedium-manual-match-ktau-native}
    % \end{table}
  \end{subfigure}%
  \begin{subfigure}{.5\linewidth}
    % \begin{table}
      \centering
      \caption{All schemas (native + modified)}
      \begin{tabular}{lllr}
\toprule
Model & $\tau$ & p & n \\
\midrule
Gem-1.5-ZS & 0.066 & 0.001 & 2012 \\
GPT-4o-ZS & 0.044 & 0.033 & 2022 \\
DINSQL & 0.070 & 0.001 & 2012 \\
GPT-3.5-ZS & 0.059 & 0.004 & 2012 \\
Ph-CdLlm2-ZS & 0.025 & 0.219 & 2012 \\
CodeS & 0.036 & 0.082 & 2012 \\
\bottomrule
\end{tabular}

      \label{table:natmedium-manual-match-ktau-all}
    % \end{table}
  \end{subfigure}
\end{table*}

\begin{table*}
  \centering
  \caption{Kendall-Tau ($\tau$) Correlations between \emph{Least Identifier Proportion} and \emph{Execution Accuracy}.}
  \begin{subfigure}{.5\linewidth}
    % \begin{table}
      \centering
      \caption{Native schemas}
      \begin{tabular}{lllr}
\toprule
Model & $\tau$ & p & n \\
\midrule
Gem-1.5-ZS & -0.170 & 0.000 & 503 \\
GPT-4o-ZS & -0.114 & 0.004 & 513 \\
DINSQL & -0.093 & 0.022 & 503 \\
GPT-3.5-ZS & -0.210 & 0.000 & 503 \\
Ph-CdLlm2-ZS & -0.168 & 0.000 & 503 \\
CodeS & -0.185 & 0.000 & 503 \\
\bottomrule
\end{tabular}

      \label{table:natlow-manual-match-ktau-native}
    % \end{table}
  \end{subfigure}%
  \begin{subfigure}{.5\linewidth}
    % \begin{table}
      \centering
      \caption{All schemas (native + modified)}
      \begin{tabular}{lllr}
\toprule
Model & $\tau$ & p & n \\
\midrule
Gem-1.5-ZS & -0.153 & 0.000 & 2012 \\
GPT-4o-ZS & -0.172 & 0.000 & 2022 \\
DINSQL & -0.142 & 0.000 & 2012 \\
GPT-3.5-ZS & -0.194 & 0.000 & 2012 \\
Ph-CdLlm2-ZS & -0.211 & 0.000 & 2012 \\
CodeS & -0.222 & 0.000 & 2012 \\
\bottomrule
\end{tabular}

      \label{table:natlow-manual-match-ktau-all}
    % \end{table}
  \end{subfigure}
\end{table*}

\begin{table*}
  \centering
  \caption{Kendall-Tau ($\tau$) Correlations between \emph{Query Combined Naturalness} and \emph{Execution Accuracy}.}
  \begin{subfigure}{.5\linewidth}
    % \begin{table}
      \centering
      \caption{Native schemas}
      \begin{tabular}{lllr}
\toprule
Model & $\tau$ & p & n \\
\midrule
Gem-1.5-ZS & 0.077 & 0.039 & 503 \\
GPT-4o-ZS & 0.115 & 0.002 & 513 \\
DINSQL & 0.059 & 0.112 & 503 \\
GPT-3.5-ZS & 0.161 & 0.000 & 503 \\
Ph-CdLlm2-ZS & 0.151 & 0.000 & 503 \\
CodeS & 0.137 & 0.000 & 503 \\
\bottomrule
\end{tabular}

      \label{table:executionktaunative}
    % \end{table}
  \end{subfigure}%
  \begin{subfigure}{.5\linewidth}
    % \begin{table}
      \centering
      \caption{All schemas (native + modified)}
      \begin{tabular}{lllr}
\toprule
Model & $\tau$ & p & n \\
\midrule
Gem-1.5-ZS & 0.096 & 0.000 & 2012 \\
GPT-4o-ZS & 0.154 & 0.000 & 2022 \\
DINSQL & 0.097 & 0.000 & 2012 \\
GPT-3.5-ZS & 0.157 & 0.000 & 2012 \\
Ph-CdLlm2-ZS & 0.196 & 0.000 & 2012 \\
CodeS & 0.197 & 0.000 & 2012 \\
\bottomrule
\end{tabular}

      \label{table:executionktauall}
    % \end{table}
  \end{subfigure}
\end{table*}



% Database level naturalness level boxplots:

\begin{figure*}
    \centering
    \begin{subfigure}{\textwidth}
      \centering
      \includegraphics[width=\textwidth]{figures/natlevel-boxplots/natlevel-model-f1-boxplot-ASIS.pdf}
    \end{subfigure}
    \begin{subfigure}{\textwidth}
      \centering
      \includegraphics[width=\textwidth]{figures/natlevel-boxplots/natlevel-model-f1-boxplot-ATBI.pdf}
    \end{subfigure}
    \begin{subfigure}{\textwidth}
      \centering
      \includegraphics[width=\textwidth]{figures/natlevel-boxplots/natlevel-model-f1-boxplot-CWO.pdf}
    \end{subfigure}
    \begin{subfigure}{\textwidth}
      \centering
      \includegraphics[width=\textwidth]{figures/natlevel-boxplots/natlevel-model-f1-boxplot-KIS.pdf}
    \end{subfigure}
    \begin{subfigure}{\textwidth}
      \centering
      \includegraphics[width=\textwidth]{figures/natlevel-boxplots/natlevel-model-f1-boxplot-NPFM.pdf}
    \end{subfigure}
    \caption{Schema linking performance (F1 score) changes across database naturalness levels.}
    \label{fig:db-level-boxplots-f1-1}
  \end{figure*}
  
  \begin{figure*}
    \centering
    \begin{subfigure}{\textwidth}
      \centering
      \includegraphics[width=\textwidth]{figures/natlevel-boxplots/natlevel-model-f1-boxplot-NTSB.pdf}
    \end{subfigure}
    \begin{subfigure}{\textwidth}
      \centering
      \includegraphics[width=\textwidth]{figures/natlevel-boxplots/natlevel-model-f1-boxplot-NYSED.pdf}
    \end{subfigure}
    \begin{subfigure}{\textwidth}
      \centering
      \includegraphics[width=\textwidth]{figures/natlevel-boxplots/natlevel-model-f1-boxplot-PILB.pdf}
    \end{subfigure}
    \begin{subfigure}{\textwidth}
      \centering
      \includegraphics[width=\textwidth]{figures/natlevel-boxplots/natlevel-model-f1-boxplot-SBOD.pdf}
    \end{subfigure}
    \caption{Schema linking performance (F1 score) changes across database naturalness levels.}
    \label{fig:db-level-boxplots-f1-2}
  \end{figure*}
  
  \begin{figure*}
    \centering
    \begin{subfigure}{\textwidth}
      \centering
      \includegraphics[width=\textwidth]{figures/natlevel-boxplots/natlevel-model-recall-boxplot-ASIS.pdf}
    \end{subfigure}
    \begin{subfigure}{\textwidth}
      \centering
      \includegraphics[width=\textwidth]{figures/natlevel-boxplots/natlevel-model-recall-boxplot-ATBI.pdf}
    \end{subfigure}
    \begin{subfigure}{\textwidth}
      \centering
      \includegraphics[width=\textwidth]{figures/natlevel-boxplots/natlevel-model-recall-boxplot-CWO.pdf}
    \end{subfigure}
    \begin{subfigure}{\textwidth}
      \centering
      \includegraphics[width=\textwidth]{figures/natlevel-boxplots/natlevel-model-recall-boxplot-KIS.pdf}
    \end{subfigure}
    \begin{subfigure}{\textwidth}
      \centering
      \includegraphics[width=\textwidth]{figures/natlevel-boxplots/natlevel-model-recall-boxplot-NPFM.pdf}
    \end{subfigure}
    \caption{Schema linking performance (Recall score) changes across database naturalness levels.}
    \label{fig:db-level-boxplots-recall-1}
  \end{figure*}
  
  \begin{figure*}
    \centering
    \begin{subfigure}{\textwidth}
      \centering
      \includegraphics[width=\textwidth]{figures/natlevel-boxplots/natlevel-model-recall-boxplot-NTSB.pdf}
    \end{subfigure}
    \begin{subfigure}{\textwidth}
      \centering
      \includegraphics[width=\textwidth]{figures/natlevel-boxplots/natlevel-model-recall-boxplot-NYSED.pdf}
    \end{subfigure}
    \begin{subfigure}{\textwidth}
      \centering
      \includegraphics[width=\textwidth]{figures/natlevel-boxplots/natlevel-model-recall-boxplot-PILB.pdf}
    \end{subfigure}
    \begin{subfigure}{\textwidth}
      \centering
      \includegraphics[width=\textwidth]{figures/natlevel-boxplots/natlevel-model-recall-boxplot-SBOD.pdf}
    \end{subfigure}
    \caption{Schema linking performance (Recall score) changes across database naturalness levels.}
    \label{fig:db-level-boxplots-recall-2}
  \end{figure*}
  




